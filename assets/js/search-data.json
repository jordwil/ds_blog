{
  
    
        "post0": {
            "title": "Learning for Gold",
            "content": "Introduction . Let&#39;s say you&#39;ve been hired by a gold extraction company. . They&#39;re interested in developing a model that will predict the recovery of gold in their gold extraction process. We will explore the dataset, determine an appropriate error metric, and find a model that minimizes this error. . # collapse from pathlib import Path import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from joblib import dump, load sns.set() from sklearn.metrics import make_scorer from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.multioutput import RegressorChain from sklearn.linear_model import SGDRegressor from sklearn.metrics import mean_absolute_error from sklearn.model_selection import RandomizedSearchCV, GridSearchCV STATE = 10 . . model_path = Path(&quot;../models&quot;) file_paths = [ Path(&quot;../datasets&quot;) / (&quot;gold_recovery_&quot; + name + &quot;.csv&quot;) for name in [&quot;train&quot;, &quot;test&quot;, &quot;full&quot;] ] train, X_test, full = (pd.read_csv(f) for f in file_paths) cats = [ &quot;rougher.input.feed&quot;, &quot;rougher.output.concentrate&quot;, &quot;rougher.output.tail&quot;, &quot;final.output.concentrate&quot;, &quot;final.output.tail&quot;, ] . The following transform allows us to separate categories into individual state, statetype and mineral to better understand how share value changes in each step. . #collapse ele_df = train.loc[:, train.columns.str.match(&quot;|&quot;.join(cats))].melt( var_name=&quot;name&quot;, value_name=&quot;share&quot; ) pattern = r&quot;(?P&lt;stage&gt; w+) .(?P&lt;statetype&gt; w+ . w+)_(?P&lt;mineral&gt; w w)&quot; ele_df = ele_df.join(ele_df[&quot;name&quot;].str.extract(pattern)).drop(&quot;name&quot;, axis=1) with sns.plotting_context(&quot;notebook&quot;, font_scale=1.5): g = sns.catplot( kind=&quot;box&quot;, hue=&quot;stage&quot;, y=&quot;share&quot;, data=ele_df, col_order=[&quot;au&quot;, &quot;ag&quot;, &quot;pb&quot;], x=&quot;statetype&quot;, col=&quot;mineral&quot;, hue_order=[&quot;rougher&quot;, &quot;final&quot;], order=[&quot;input.feed&quot;, &quot;output.concentrate&quot;, &quot;output.tail&quot;], ) g.set_xticklabels(rotation=30) . . Here we can observe how the process creating a higher share of gold as it is processed, while the other minerals have a higher share in the tails. . There also seems to be some long tails with the output.concentrate of au. It&#39;s worth interrogating these outliers. . # collapse ele_df.loc[ele_df[&quot;mineral&quot;] == &quot;au&quot;].groupby( [&quot;statetype&quot;, &quot;stage&quot;, &quot;mineral&quot;] ).describe() . . share . count mean std min 25% 50% 75% max . statetype stage mineral . input.feed rougher au 16777.0 | 7.170717 | 3.002113 | 0.000000 | 6.203083 | 7.443530 | 8.965193 | 14.093363 | . output.concentrate final au 16789.0 | 39.467217 | 13.917227 | 0.000000 | 42.055722 | 44.498874 | 45.976222 | 53.611374 | . rougher au 16778.0 | 17.401541 | 6.921875 | 0.000000 | 17.443663 | 19.644663 | 21.224486 | 28.522811 | . output.tail final au 16794.0 | 2.687512 | 1.272757 | 0.000000 | 2.172953 | 2.781132 | 3.416936 | 9.789625 | . rougher au 14611.0 | 1.763930 | 0.731518 | 0.020676 | 1.311610 | 1.747907 | 2.190839 | 9.688980 | . train.columns[train.columns.str.endswith(&quot;au&quot;)] . Index([&#39;final.output.concentrate_au&#39;, &#39;final.output.tail_au&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;rougher.input.feed_au&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_au&#39;], dtype=&#39;object&#39;) . C - share of gold in concentrate right after flotation - _rougher.output.concentrateau . F - share of gold in the feed before flotation - _rougher.input.feedau . T - share of gold in rougher tails right after flotation - _rougher.output.tailau . Find share of gold in each state . def calc_recovery(C, F, T, conv_inf=True, conv_neg=True, conv_high=True): rec = ((C * (F - T)) / (F * (C - T))) * 100 if conv_inf: rec[np.abs(rec) == np.inf] = np.nan if conv_neg: rec[rec &lt; 0] = np.nan if conv_high: rec[rec &gt; 100] = np.nan return rec . Features not available on the test set: . Most of these features are describing the output, which would be information we wouldn&#39;t have when trying to predict the output of AU in each stage. . # Examining the difference in columns between the training and testing set. features = X_test.columns train.loc[:, sorted(set(train.columns).difference(set(features)))].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 34 columns): # Column Non-Null Count Dtype -- -- 0 final.output.concentrate_ag 16788 non-null float64 1 final.output.concentrate_au 16789 non-null float64 2 final.output.concentrate_pb 16788 non-null float64 3 final.output.concentrate_sol 16490 non-null float64 4 final.output.recovery 15339 non-null float64 5 final.output.tail_ag 16794 non-null float64 6 final.output.tail_au 16794 non-null float64 7 final.output.tail_pb 16677 non-null float64 8 final.output.tail_sol 16715 non-null float64 9 primary_cleaner.output.concentrate_ag 16778 non-null float64 10 primary_cleaner.output.concentrate_au 16778 non-null float64 11 primary_cleaner.output.concentrate_pb 16502 non-null float64 12 primary_cleaner.output.concentrate_sol 16224 non-null float64 13 primary_cleaner.output.tail_ag 16777 non-null float64 14 primary_cleaner.output.tail_au 16777 non-null float64 15 primary_cleaner.output.tail_pb 16761 non-null float64 16 primary_cleaner.output.tail_sol 16579 non-null float64 17 rougher.calculation.au_pb_ratio 15618 non-null float64 18 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 19 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 20 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 21 rougher.output.concentrate_ag 16778 non-null float64 22 rougher.output.concentrate_au 16778 non-null float64 23 rougher.output.concentrate_pb 16778 non-null float64 24 rougher.output.concentrate_sol 16698 non-null float64 25 rougher.output.recovery 14287 non-null float64 26 rougher.output.tail_ag 14610 non-null float64 27 rougher.output.tail_au 14611 non-null float64 28 rougher.output.tail_pb 16778 non-null float64 29 rougher.output.tail_sol 14611 non-null float64 30 secondary_cleaner.output.tail_ag 16776 non-null float64 31 secondary_cleaner.output.tail_au 16778 non-null float64 32 secondary_cleaner.output.tail_pb 16764 non-null float64 33 secondary_cleaner.output.tail_sol 14874 non-null float64 dtypes: float64(34) memory usage: 4.4 MB . test = full.set_index(&quot;date&quot;).loc[X_test[&quot;date&quot;]].reset_index() . train.shape . (16860, 87) . datasets = [train, test] types = { &quot;recovery_first&quot;: [ &quot;rougher.output.concentrate_au&quot;, &quot;rougher.input.feed_au&quot;, &quot;rougher.output.tail_au&quot;, ], &quot;recovery_final&quot;: [ &quot;final.output.concentrate_au&quot;, &quot;rougher.output.concentrate_au&quot;, &quot;final.output.tail_au&quot;, ], } for d in datasets: for name, cols in types.items(): d[name] = calc_recovery(*[d[t] for t in cols]) . In order for the model to work correctly, we need to drop NA values from our target variables. . train = train.loc[~train[types.keys()].isna().apply(any, axis=1)] test = test.loc[~test[types.keys()].isna().apply(any, axis=1)] . I&#39;m dropping the date feature, as it will not serve to improve the model. . X_train, y_train = ( train[features].drop(&quot;date&quot;, axis=1), train[[&quot;recovery_first&quot;, &quot;recovery_final&quot;]], ) X_test, y_test = ( test[features].drop(&quot;date&quot;, axis=1), test[[&quot;recovery_first&quot;, &quot;recovery_final&quot;]], ) . Check to ensure that it matches with calculated recovery . I&#39;m checking to see if there are any values that aren&#39;t matching the recorded output recovery. . unusual_values = train.loc[ train[&quot;recovery_first&quot;].round(2) != train[&quot;rougher.output.recovery&quot;].round(2), [&quot;recovery_first&quot;, &quot;rougher.output.recovery&quot;], ] . unusual_values.describe() . recovery_first rougher.output.recovery . count 0.0 | 0.0 | . mean NaN | NaN | . std NaN | NaN | . min NaN | NaN | . 25% NaN | NaN | . 50% NaN | NaN | . 75% NaN | NaN | . max NaN | NaN | . Cleaning the unusual values . train = train.loc[train.index.difference(unusual_values.index), :] . There&#39;s a long tail distribtion, though I&#39;m uncertain if these values are a true phenomenon. . # collapse sns.boxplot(train[&quot;recovery_first&quot;]) plt.show() . . There&#39;s virtually no difference between the calculated values. . mean_absolute_error(train[&quot;recovery_first&quot;], train[&quot;rougher.output.recovery&quot;]) . 9.663190048480168e-15 . # collapse sns.boxplot(train[&quot;recovery_final&quot;]) plt.show() . . dfs = [] for group, col_names in types.items(): vis_df = pd.melt( train[col_names], var_name=&quot;Extract Stage&quot;, value_name=&quot;Gold Concentration&quot; ) vis_df[&quot;group&quot;] = group dfs.append(vis_df) vis_df = pd.concat(dfs) . vis_df[&quot;Extract Stage&quot;] = ( vis_df[&quot;Extract Stage&quot;] .str.replace(&quot;final.output.concentrate_au&quot;, &quot;output&quot;) .str.replace(&quot;rougher.output.tail_au&quot;, &quot;tails&quot;) .str.replace(&quot;final.output.tail_au&quot;, &quot;tails&quot;) .str.replace(&quot;rougher.input.feed_au&quot;, &quot;input&quot;) ) vis_df.loc[ (vis_df[&quot;group&quot;] == &quot;recovery_final&quot;) &amp; (vis_df[&quot;Extract Stage&quot;] == &quot;rougher.output.concentrate_au&quot;), &quot;Extract Stage&quot;, ] = ( &quot;input&quot; ) vis_df[&quot;Extract Stage&quot;] = vis_df[&quot;Extract Stage&quot;].str.replace( &quot;rougher.output.concentrate_au&quot;, &quot;output&quot; ) order = [&quot;input&quot;, &quot;output&quot;, &quot;tails&quot;] . It looks like the final stage is much more effective at increasing the share of gold in our solution. . Because the second process is so much more effective, We can give more weight to the model that can give us a stronger prediction of a high yield at the final stage, though it&#39;s still important to properly extract gold in the first phase. . # collapse with sns.plotting_context(&quot;notebook&quot;, font_scale=1.5): g = sns.catplot( x=&quot;group&quot;, y=&quot;Gold Concentration&quot;, data=vis_df, col=&quot;Extract Stage&quot;, kind=&quot;box&quot;, col_order=order, ) . . We can now compare metal shares throughout the purification process. . #collapse ele_df = train.loc[:, train.columns.str.match(&quot;|&quot;.join(cats))].melt( var_name=&quot;name&quot;, value_name=&quot;share&quot; ) pattern = r&quot;(?P&lt;stage&gt; w+) .(?P&lt;statetype&gt; w+ . w+)_(?P&lt;mineral&gt; w w)&quot; ele_df = ele_df.join(ele_df[&quot;name&quot;].str.extract(pattern)).drop(&quot;name&quot;, axis=1) with sns.plotting_context(&quot;notebook&quot;, font_scale=1.5): g = sns.catplot( kind=&quot;box&quot;, hue=&quot;stage&quot;, y=&quot;share&quot;, data=ele_df, col_order=[&quot;au&quot;, &quot;ag&quot;, &quot;pb&quot;], x=&quot;statetype&quot;, col=&quot;mineral&quot;, hue_order=[&quot;rougher&quot;, &quot;final&quot;], order=[&quot;input.feed&quot;, &quot;output.concentrate&quot;, &quot;output.tail&quot;], ) g.set_xticklabels(rotation=30) . . scatter_dat = train.loc[:, train.columns.str.match(&quot;|&quot;.join(cats))] with sns.plotting_context(&quot;notebook&quot;, font_scale=1.5): fig, ax = plt.subplots(figsize=[10,10]) g = sns.scatterplot(data=scatter_dat, x=&quot;rougher.input.feed_au&quot;, y=&quot;rougher.output.concentrate_au&quot;, ax=ax, alpha=0.25) plt.show() fig, ax = plt.subplots(figsize=[10,10]) g = sns.scatterplot(data=scatter_dat, x=&quot;rougher.output.concentrate_au&quot;, y=&quot;final.output.concentrate_au&quot;, ax=ax, alpha=0.25) plt.show() . When comparing feed size distribution between training and testing, I don&#39;t see any major differences in the distributions. . # collapse comp_feed = pd.concat( [train[&quot;rougher.input.feed_size&quot;], test[&quot;rougher.input.feed_size&quot;]], axis=1 ) comp_feed.columns = [&quot;train&quot;, &quot;test&quot;] comp_feed = pd.melt(comp_feed, var_name=&quot;rougher input feed size&quot;, value_name=&quot;share&quot;) sns.boxplot(data=comp_feed, x=&quot;rougher input feed size&quot;, y=&quot;share&quot;) plt.show() . . # collapse train[&quot;recovery_first&quot;].describe() sns.boxplot(train[&quot;recovery_first&quot;].dropna()) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcd46a7ab20&gt; . def calc_smape(y: np.array, y_pred: np.array) -&gt; float: &quot;&quot;&quot;Symmetric Mean Absolute Percentage Error&quot;&quot;&quot; smape_s = ( np.mean( ( np.abs(y_pred - y) / ((np.abs(y) + np.abs(y_pred)) / 2) ) ) * 100 ) return smape_s def smape_score(y: np.array, y_pred: np.array) -&gt; float: &quot;&quot;&quot;Combine the &quot;&quot;&quot; smape_r = calc_smape(y[:, 0], y_pred[:, 0]) smape_f = calc_smape(y[:, 1], y_pred[:, 1]) final_score = smape_r * 0.25 + smape_f * 0.75 return final_score . y_train.head() . recovery_first recovery_final . 0 87.107763 | 93.944554 | . 1 86.843261 | 93.790501 | . 2 86.842308 | 93.509750 | . 3 87.226430 | 93.595268 | . 4 86.688794 | 93.811976 | . X_train.head() . primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air primary_cleaner.state.floatbank8_c_level ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 127.092003 | 10.128295 | 7.25 | 0.988759 | 1549.775757 | -498.912140 | 1551.434204 | -516.403442 | 1549.873901 | -498.666595 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 125.629232 | 10.296251 | 7.25 | 1.002663 | 1576.166671 | -500.904965 | 1575.950626 | -499.865889 | 1575.994189 | -499.315107 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 123.819808 | 11.316280 | 7.25 | 0.991265 | 1601.556163 | -499.997791 | 1600.386685 | -500.607762 | 1602.003542 | -500.870069 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 122.270188 | 11.322140 | 7.25 | 0.996739 | 1599.968720 | -500.951778 | 1600.659236 | -499.677094 | 1600.304144 | -500.727997 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 117.988169 | 11.913613 | 7.25 | 1.009869 | 1601.339707 | -498.975456 | 1601.437854 | -500.323246 | 1599.581894 | -500.888152 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 52 columns . Now that we have a preprocessing plan, lets experiment with two algorithms. . - A stochastic gradient decent algorithm with a grid search of reasonable hyperparameters. - A random forest with 200 estimators. . prep = make_pipeline(SimpleImputer(), StandardScaler()) rf = make_pipeline(SimpleImputer(), RandomForestRegressor(random_state=STATE)) lin = make_pipeline(prep, MultiOutputRegressor(SGDRegressor(random_state=STATE))) . Here&#39;s what the grid search looks like. Luckly, SGD can be very quick and we can rapidly search through our parameters. . #collapse lhparams = { &quot;multioutputregressor__estimator__alpha&quot;: 10.0 ** -np.arange(1, 7), &quot;multioutputregressor__estimator__penalty&quot;: [&quot;l2&quot;, &quot;l1&quot;, &quot;elasticnet&quot;], &quot;multioutputregressor__estimator__loss&quot;: [ &quot;huber&quot;, &quot;squared_loss&quot;, &quot;epsilon_insensitive&quot;, ], &quot;pipeline__simpleimputer__strategy&quot;: [&quot;mean&quot;, &quot;median&quot;], } try: lin_search = load(model_path / &quot;lin_search.joblib&quot;) except: lin_search = GridSearchCV( lin, lhparams, scoring=make_scorer(smape_score, greater_is_better=False), cv=5, n_jobs=-1, ) lin_search.fit(X_train, y_train.values) dump(lin_search, model_path / &quot;lin_search.joblib&quot;) print(f&quot;Our best linear model produced a symmetric mean absolute error of {-lin_search.best_score_:.2f} percent on the mean validation score&quot;) . . Our best linear model produced a symmetric mean absolute error of 3.76 percent on the mean validation score . We can see the params from our search as well: . #collapse lin_search.best_params_ . . {&#39;multioutputregressor__estimator__alpha&#39;: 0.1, &#39;multioutputregressor__estimator__loss&#39;: &#39;epsilon_insensitive&#39;, &#39;multioutputregressor__estimator__penalty&#39;: &#39;l2&#39;, &#39;pipeline__simpleimputer__strategy&#39;: &#39;median&#39;} . To see if it&#39;s worth experimenting with a random forest algorithm, let&#39;s try using one and seeing how well it compares to our best linear model. . from sklearn.model_selection import cross_validate rf_pipe = make_pipeline( SimpleImputer(), RandomForestRegressor(n_estimators=200, random_state=STATE) ) rf_cv = cross_validate( rf_pipe, X=X_train, y=y_train.values, cv=5, scoring=make_scorer(smape_score, greater_is_better=False), n_jobs=-1, ) . The random forest regressor isn&#39;t producing very good validation scores. Let&#39;s stick with our best linear model. . rf_cv . {&#39;fit_time&#39;: array([302.93810821, 300.83394909, 283.57908678, 303.35128713, 147.72976899]), &#39;score_time&#39;: array([0.47671795, 0.34891081, 0.36139321, 0.32253885, 0.10421824]), &#39;test_score&#39;: array([-5.97799642, -6.90910333, -4.97451007, -4.99227667, -5.95214758])} . test_score = lin_search.score(X_test, y_test.values) print(f&quot;&quot;&quot;Our best linear model produced a symmetric mean absolute error of: {-test_score:.2f} percent on our testing dataset&quot;&quot;&quot;) . Our best linear model produced a symmetric mean absolute error of: 3.65 percent on our testing dataset .",
            "url": "https://jordwil.github.io/ds_blog/machine%20learning/python/multi-target/regression/2020/07/30/int-project-gold.html",
            "relUrl": "/machine%20learning/python/multi-target/regression/2020/07/30/int-project-gold.html",
            "date": " • Jul 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "SQL Practice",
            "content": ". Let&#39;s solve a mystery hidden in an sql database. . The instructions and database was created by Joon Park and Cathy He while fellows at the Knight Lab at Northwesten University. . Here&#39;s a step by step solution, though if you&#39;d like to try it yourself first, check out their interactive site. . Here&#39;s the description from the knight lab: . A crime has taken place and the detective needs your help. The detective gave you the crime scene report, but you somehow lost it. You vaguely remember that the crime was a murder that occurred sometime on Jan.15, 2018 and that it took place in SQL City. Start by retrieving the corresponding crime scene report from the police department’s database. . The setup . We&#39;ll use pandas to make the queries look nice and sqlite3 to make the connection. . I&#39;m also adding a function pq to quickly grab the results of a query based on the value of the query variable. . #collapse import pandas as pd import sqlite3 from sqlite3 import Error from pathlib import Path . . def create_connection(path): connection = None try: connection = sqlite3.connect(path) print(&quot;Connection to SQLite DB successful&quot;) except Error as e: print(f&quot;The error &#39;{e}&#39; occurred&quot;) return connection . db = Path(&quot;../datasets/sql-murder-mystery.db&quot;) conn = create_connection(db) . Connection to SQLite DB successful . Taking a look at all the tables by selecting name filtering by table. . def pq(query, conn=conn): &quot;&quot;&quot;Returns a sql query as a pandas dataframe&quot;&quot;&quot; return pd.read_sql(query, conn) . query = &quot;&quot;&quot;SELECT name FROM sqlite_master where type = &#39;table&#39;&quot;&quot;&quot; pq(query) . name . 0 crime_scene_report | . 1 drivers_license | . 2 person | . 3 facebook_event_checkin | . 4 interview | . 5 get_fit_now_member | . 6 get_fit_now_check_in | . 7 income | . 8 solution | . In order to determine the structure of each table, we can run a query like this: . query = &quot;&quot;&quot; SELECT sql FROM sqlite_master WHERE name = &#39;crime_scene_report&#39; &quot;&quot;&quot; print(pq(query).iloc[0, 0]) . CREATE TABLE crime_scene_report ( date integer, type text, description text, city text ) . Now that we have the database loaded and the query print function, we&#39;re ready to tackle this murder mystery. . Solving the mystery . Finding the witnesses . query = &quot;&quot;&quot; SELECT * FROM crime_scene_report LIMIT 5&quot;&quot;&quot; pq(query) . date type description city . 0 20180115 | robbery | A Man Dressed as Spider-Man Is on a Robbery Spree | NYC | . 1 20180115 | murder | Life? Dont talk to me about life. | Albany | . 2 20180115 | murder | Mama, I killed a man, put a gun against his he... | Reno | . 3 20180215 | murder | REDACTED REDACTED REDACTED | SQL City | . 4 20180215 | murder | Someone killed the guard! He took an arrow to ... | SQL City | . Let&#39;s start with what we know. According to the prompt, we know that this murder took place on Jan. 15 2018 in SQL City. This should be enough info to explore crime scene reports. . We care about the description, and want to filter by city, date, and type. . query = &quot;&quot;&quot; SELECT description FROM crime_scene_report WHERE city = &#39;SQL City&#39; AND type = &#39;murder&#39; AND date = 20180115&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . Security footage shows that there were 2 witnesses. The first witness lives at the last house on &#34;Northwestern Dr&#34;. The second witness, named Annabel, lives somewhere on &#34;Franklin Ave&#34;. . Finding the interviews . Located witnesses presumably interviewed by detectives. Let&#39;s take a quick look the interview table . query = &quot;&quot;&quot; SELECT * FROM interview LIMIT 5 &quot;&quot;&quot; pq(query) . person_id transcript . 0 28508 | ‘I deny it!’ said the March Hare. n | . 1 63713 | n | . 2 86208 | way, and the whole party swam to the shore. n | . 3 35267 | lessons in here? Why, there’s hardly room for ... | . 4 33856 | n | . Not too useful (yet). We need to figure out how the person_id connects . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;interview&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE interview ( person_id integer, transcript text, FOREIGN KEY (person_id) REFERENCES person(id) ) . AHA! So let&#39;s connect person_id from the person table. . #collapse query = &quot;&quot;&quot; SELECT sql FROM sqlite_master WHERE name = &#39;person&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE person ( id integer PRIMARY KEY, name text, license_id integer, address_number integer, address_street_name text, ssn integer, FOREIGN KEY (license_id) REFERENCES drivers_license(id) ) . The clues we have are: . Annabel, who lives on &quot;Franklin Ave&quot; | Someone else who lives on the last house on &quot;Northwestern Dr&quot; | . Once we join both tables, a subquery will help capture the last house by finding the largest street number for Northwestern Dr. We can use an fstring to inject the subquery, making it a bit more readable. . subq = &quot;&quot;&quot; SELECT MAX(address_number) FROM person WHERE address_street_name = &#39;Northwestern Dr&#39; &quot;&quot;&quot; query = f&quot;&quot;&quot; SELECT transcript FROM person AS p INNER JOIN interview AS i ON p.id = i.person_id WHERE (address_street_name = &#39;Franklin Ave&#39; AND name LIKE &#39;Annabel%&#39;) OR (address_street_name = &#39;Northwestern Dr&#39; AND address_number IN ({subq})); &quot;&quot;&quot; [print(f&quot;{v} n&quot;) for v in pq(query)[&quot;transcript&quot;]] . I heard a gunshot and then saw a man run out. He had a &#34;Get Fit Now Gym&#34; bag. The membership number on the bag started with &#34;48Z&#34;. Only gold members have those bags. The man got into a car with a plate that included &#34;H42W&#34;. I saw the murder happen, and I recognized the killer from my gym when I was working out last week on January the 9th. . [None, None] . Examining the crime scene . Let&#39;s extract the salient points: . Suspect was at the gym on Jan 9th . | Suspect (same?) male and has Get Fit Now Bag, presumably a member with &quot;48Z&quot; as the starting membership id. Gold member bag. Car connected to suspect has plate number partial of &quot;H42W&quot; . | . Taking a look at gym data: . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;get_fit_now_check_in&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE get_fit_now_check_in ( membership_id text, check_in_date integer, check_in_time integer, check_out_time integer, FOREIGN KEY (membership_id) REFERENCES get_fit_now_member(id) ) . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;get_fit_now_member&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE get_fit_now_member ( id text PRIMARY KEY, person_id integer, name text, membership_start_date integer, membership_status text, FOREIGN KEY (person_id) REFERENCES person(id) ) . query = &quot;&quot;&quot; SELECT * FROM get_fit_now_check_in WHERE check_in_date = 20180109 AND membership_id LIKE &#39;48Z%&#39; &quot;&quot;&quot; pq(query) . membership_id check_in_date check_in_time check_out_time . 0 48Z7A | 20180109 | 1600 | 1730 | . 1 48Z55 | 20180109 | 1530 | 1700 | . This is useful. Let&#39;s see if we can combine this with the info from the other witness to isolate a single suspect. . subq = &quot;&quot;&quot; SELECT * FROM get_fit_now_check_in WHERE check_in_date = 20180109 AND membership_id LIKE &#39;48Z%&#39; &quot;&quot;&quot; query = f&quot;&quot;&quot; SELECT * FROM ({subq}) AS c INNER JOIN get_fit_now_member AS m ON m.id = c.membership_id &quot;&quot;&quot; pq(query) . membership_id check_in_date check_in_time check_out_time id person_id name membership_start_date membership_status . 0 48Z7A | 20180109 | 1600 | 1730 | 48Z7A | 28819 | Joe Germuska | 20160305 | gold | . 1 48Z55 | 20180109 | 1530 | 1700 | 48Z55 | 67318 | Jeremy Bowers | 20160101 | gold | . We can take advantage of the &#39;free&#39; filtration by performing an inner join on the membership id. . Unfortunately we&#39;re stuck between two people. Our last resort is the driver&#39;s license. . Finding the murderer . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;drivers_license&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE drivers_license ( id integer PRIMARY KEY, age integer, height integer, eye_color text, hair_color text, gender text, plate_number text, car_make text, car_model text ) . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;person&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE person ( id integer PRIMARY KEY, name text, license_id integer, address_number integer, address_street_name text, ssn integer, FOREIGN KEY (license_id) REFERENCES drivers_license(id) ) . Lots of interesting info in these tables, so lets select everything that has a pretty good match. . query = &quot;&quot;&quot; SELECT * FROM drivers_license WHERE plate_number LIKE &#39;%H42W%&#39; AND gender = &#39;male&#39; &quot;&quot;&quot; pq(query) . id age height eye_color hair_color gender plate_number car_make car_model . 0 423327 | 30 | 70 | brown | brown | male | 0H42W2 | Chevrolet | Spark LS | . 1 664760 | 21 | 71 | black | black | male | 4H42WR | Nissan | Altima | . Interesting... lets see if it matches either of our gym members. . subq = &quot;&quot;&quot; SELECT * FROM drivers_license WHERE plate_number LIKE &#39;%H42W%&#39; AND gender = &#39;male&#39; &quot;&quot;&quot; query = f&quot;&quot;&quot; SELECT * FROM ({subq}) d INNER JOIN person p ON d.id = p.license_id&quot;&quot;&quot; pq(query) . id age height eye_color hair_color gender plate_number car_make car_model id name license_id address_number address_street_name ssn . 0 664760 | 21 | 71 | black | black | male | 4H42WR | Nissan | Altima | 51739 | Tushar Chandra | 664760 | 312 | Phi St | 137882671 | . 1 423327 | 30 | 70 | brown | brown | male | 0H42W2 | Chevrolet | Spark LS | 67318 | Jeremy Bowers | 423327 | 530 | Washington Pl, Apt 3A | 871539279 | . Ah! We found the suspect in the list, but let&#39;s create a query that combines it all. . g_subq = &quot;&quot;&quot; SELECT * FROM get_fit_now_check_in WHERE check_in_date = 20180109 AND membership_id LIKE &#39;48Z%&#39; &quot;&quot;&quot; g_query = f&quot;&quot;&quot; SELECT name FROM ({g_subq}) AS c INNER JOIN get_fit_now_member AS m ON m.id = c.membership_id &quot;&quot;&quot; subq = &quot;&quot;&quot; SELECT * FROM drivers_license WHERE plate_number LIKE &#39;%H42W%&#39; AND gender = &#39;male&#39; &quot;&quot;&quot; query = f&quot;&quot;&quot; SELECT * FROM ({subq}) d INNER JOIN person p ON d.id = p.license_id WHERE name IN ({g_query}) &quot;&quot;&quot; pq(query) . id age height eye_color hair_color gender plate_number car_make car_model id name license_id address_number address_street_name ssn . 0 423327 | 30 | 70 | brown | brown | male | 0H42W2 | Chevrolet | Spark LS | 67318 | Jeremy Bowers | 423327 | 530 | Washington Pl, Apt 3A | 871539279 | . Perhaps we found the murderer? Let&#39;s see if there&#39;s anything else we can find out about the suspect. Perhaps he was interviewed? . query = &quot;&quot;&quot; SELECT * FROM interview WHERE person_id = 67318 &quot;&quot;&quot; print(pq(query).iloc[0, 1]) . I was hired by a woman with a lot of money. I don&#39;t know her name but I know she&#39;s around 5&#39;5&#34; (65&#34;) or 5&#39;7&#34; (67&#34;). She has red hair and she drives a Tesla Model S. I know that she attended the SQL Symphony Concert 3 times in December 2017. . It turns out we&#39;re not done yet! . Finding the real murderer (bonus round) . Between 65&quot; AND 67&quot; | Red hair | High income | Tesla Model S | Checkin SQL Symphony Concert 3 times in December 2017 | . query = &quot;&quot;&quot; SELECT * FROM drivers_license WHERE height BETWEEN 65 AND 67 AND car_make = &#39;Tesla&#39; AND gender = &#39;female&#39; AND hair_color = &#39;red&#39; &quot;&quot;&quot; pq(query) . id age height eye_color hair_color gender plate_number car_make car_model . 0 202298 | 68 | 66 | green | red | female | 500123 | Tesla | Model S | . 1 291182 | 65 | 66 | blue | red | female | 08CM64 | Tesla | Model S | . 2 918773 | 48 | 65 | black | red | female | 917UU3 | Tesla | Model S | . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;facebook_event_checkin&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE facebook_event_checkin ( person_id integer, event_id integer, event_name text, date integer, FOREIGN KEY (person_id) REFERENCES person(id) ) . #collapse query = &quot;&quot;&quot;SELECT sql FROM sqlite_master where name = &#39;income&#39;&quot;&quot;&quot; print(pq(query).iloc[0, 0]) . . CREATE TABLE income ( ssn integer PRIMARY KEY, annual_income integer ) . We can create a single query and combine all . query = &quot;&quot;&quot; SELECT name, annual_income FROM facebook_event_checkin f INNER JOIN person p ON p.id = f.person_id INNER JOIN drivers_license d ON p.license_id = d.id INNER JOIN income i ON p.ssn = i.ssn WHERE date BETWEEN 20171200 AND 20171231 AND event_name = &#39;SQL Symphony Concert&#39; AND height BETWEEN 65 AND 67 AND hair_color = &#39;red&#39; AND gender = &#39;female&#39; AND car_make = &#39;Tesla&#39; AND car_model = &#39;Model S&#39; GROUP BY person_id HAVING COUNT(person_id) &gt; 2 &quot;&quot;&quot; pq(query) . name annual_income . 0 Miranda Priestly | 310000 | . Looks like a match! .",
            "url": "https://jordwil.github.io/ds_blog/sql/python/2020/06/30/sql-fun.html",
            "relUrl": "/sql/python/2020/06/30/sql-fun.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How risky is my oil field?",
            "content": "You work for the OilyGiant mining company. Your task is to find the best place for a new well. Steps to choose the location: . Collect the oil well parameters in the selected region: oil quality and volume of reserves; Build a model for predicting the volume of reserves in the new wells; Pick the oil wells with the highest estimated values; Pick the region with the highest total profit for the selected oil wells. . You have data on oil samples from three regions. Parameters of each oil well in the region are already known. Build a model that will help to pick the region with the highest profit margin. Analyze potential profit and risks using the bootstrapping technique. . Geological exploration data for the three regions are stored in files: . geo_data_0.csv. | geo_data_1.csv. | geo_data_2.csv. | id — unique oil well identifier | f0, f1, f2 — three features of points (their specific meaning is unimportant, but the features themselves are significant) | product — volume of reserves in the oil well (thousand barrels). | . Conditions: . Only linear regression is suitable for model training (the rest are not sufficiently predictable). | When exploring the region, a study of 500 points is carried with picking the best 200 points for the profit calculation. | The budget for development of 200 oil wells is 100 USD million. | One barrel of raw materials brings 4.5 USD of revenue The revenue from one unit of product is 4,500 dollars (volume of reserves is in thousand barrels). | After the risk evaluation, keep only the regions with the risk of losses lower than 2.5%. From the ones that fit the criteria, the region with the highest average profit should be selected. | . Note this is synthetic data. | . Download and prepare the data. . import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error import seaborn as sns import matplotlib.pyplot as plt sns.set() . state = np.random.RandomState(42) . g1 = pd.read_csv(&quot;geo_data_0.csv&quot;) g2 = pd.read_csv(&quot;geo_data_1.csv&quot;) g3 = pd.read_csv(&quot;geo_data_2.csv&quot;) . g1.head() . id f0 f1 f2 product . 0 | txEyH | 0.705745 | -0.497823 | 1.221170 | 105.280062 | . 1 | 2acmU | 1.334711 | -0.340164 | 4.365080 | 73.037750 | . 2 | 409Wp | 1.022732 | 0.151990 | 1.419926 | 85.265647 | . 3 | iJLyR | -0.032172 | 0.139033 | 2.978566 | 168.620776 | . 4 | Xdl7t | 1.988431 | 0.155413 | 4.751769 | 154.036647 | . g1.isna().sum() . id 0 f0 0 f1 0 f2 0 product 0 dtype: int64 . g2.isna().sum() . id 0 f0 0 f1 0 f2 0 product 0 dtype: int64 . g3.isna().sum() . id 0 f0 0 f1 0 f2 0 product 0 dtype: int64 . No missing values here! . Procedure . For each region, I&#39;ll fit a least squares regression and determine how well it&#39;s able to predict . Train and test the model for each region: Split the data into a training set and validation set at a ratio of 75:25. | Train the model and make predictions for the validation set. | Save the predictions and correct answers for the validation set. | Print the average volume of predicted reserves and model RMSE. | Analyze the results. | . | def vol_to_profit(vol): total_vol = sum(vol) price_per_1k_b = 4500 budget_per_200_wells = 100e6 profit = (total_vol * price_per_1k_b) - budget_per_200_wells return profit def bootstrap_top_sum(features, target, preds, iters=10000, n=500, top=200): exp = [] for i in range(iters): boot_feat = features.sample(n=n, random_state=state, replace=True) if isinstance(preds, pd.Series): boot_preds = preds.loc[boot_feat.index].sort_values(ascending=False) else: boot_preds = target.loc[boot_feat.index] top_preds = boot_preds.iloc[:top] top_targs = target.loc[top_preds.index] total_profit = vol_to_profit(top_targs) exp.append(total_profit) return exp . exp_model = {} exp_rand = {} for name, oil_data in zip([&quot;g1&quot;, &quot;g2&quot;, &quot;g3&quot;], [g1, g2, g3]): X_train, X_val, y_train, y_val = train_test_split( oil_data.drop([&quot;id&quot;, &quot;product&quot;], axis=1), oil_data[&quot;product&quot;], random_state=state, ) model = LinearRegression(normalize=True) model.fit(X_train, y_train) preds = pd.Series(model.predict(X_val), index=X_val.index) mean_preds = preds.mean() rmse = mean_squared_error(y_val, preds) ** (0.5) price_per_1k_b = 4500 well_rev = mean_preds * price_per_1k_b print( f&quot;&quot;&quot;For {name}, the model predicts a mean well volume in 1000s of barrels of {mean_preds:.2f} with an RMSE of {rmse:.2f}, and with a mean revenue of ${well_rev:.2f}.&quot;&quot;&quot; ) [print(f&quot;Feature f{i} coefficient is: {model.coef_[i]} n&quot;) for i in range(3)] exp_model[name] = bootstrap_top_sum(features=X_val, target=y_val, preds=preds) exp_rand[name] = bootstrap_top_sum(features=X_val, target=y_val, preds=None) . For g1, the model predicts a mean well volume in 1000s of barrels of 92.42 with an RMSE of 37.58, and a mean revenue of $415911.84. Feature f0 coefficient is: 3.856562785081632 Feature f1 coefficient is: -14.034893063209786 Feature f2 coefficient is: 6.601779095490511 For g2, the model predicts a mean well volume in 1000s of barrels of 68.49 with an RMSE of 0.89, and a mean revenue of $308204.03. Feature f0 coefficient is: -0.14510251913349947 Feature f1 coefficient is: -0.02166924119469456 Feature f2 coefficient is: 26.95259013071809 For g3, the model predicts a mean well volume in 1000s of barrels of 95.20 with an RMSE of 40.01, and a mean revenue of $428380.52. Feature f0 coefficient is: -0.04463676028315076 Feature f1 coefficient is: 0.048974423921008044 Feature f2 coefficient is: 5.744952417003429 . Prepare for profit calculation: Store all key values for calculations in separate variables. | Calculate the volume of reserves sufficient for developing a new well without losses. Compare the obtained value with the average volume of reserves in each region. | Provide the findings about the preparation for profit calculation step. | . | dfs = [] for exp_name, exp in zip([&quot;Linear Model&quot;, &quot;Random Choice&quot;], [exp_model, exp_rand]): df = pd.DataFrame(exp) df = pd.melt(df, var_name=&quot;Region&quot;, value_name=&quot;Profit (USD)&quot;) df[&quot;type&quot;] = exp_name dfs.append(df) concat_df = pd.concat(dfs) . Here&#39;s the increase in profit using a linear model compared to the random choice. . ( concat_df.groupby([&quot;Region&quot;, &quot;type&quot;]) .median() .groupby(&quot;Region&quot;)[&quot;Profit (USD)&quot;] .apply(lambda x: x.iloc[0] - x.iloc[1]) ) . Region g1 2.088452e+07 g2 4.246631e+07 g3 1.851712e+07 Name: Profit (USD), dtype: float64 . plt.figure(figsize=[20, 20]) sns.boxplot(data=concat_df, x=&quot;Region&quot;, y=&quot;Profit (USD)&quot;, hue=&quot;type&quot;) plt.title( &quot;Bootstrapped linear model choice vs. random choice of 200 out of 500 wells for each region.&quot; ) plt.show() . When exploring the region, a study of 500 points is carried with picking the best 200 points for the profit calculation. . concat_df.groupby([&quot;Region&quot;, &quot;type&quot;]).describe() . Profit (USD) . count mean std min 25% 50% 75% max . Region type . g1 | Linear Model | 10000.0 | 3.882622e+06 | 2.611796e+06 | -6.267495e+06 | 2.086072e+06 | 3.920459e+06 | 5.655827e+06 | 1.284237e+07 | . Random Choice | 10000.0 | -1.695254e+07 | 2.781690e+06 | -2.622152e+07 | -1.885831e+07 | -1.696407e+07 | -1.507457e+07 | -6.748821e+06 | . g2 | Linear Model | 10000.0 | 4.151066e+06 | 2.064010e+06 | -3.128114e+06 | 2.772162e+06 | 4.142425e+06 | 5.504601e+06 | 1.174013e+07 | . Random Choice | 10000.0 | -3.833549e+07 | 2.919280e+06 | -4.845562e+07 | -4.032921e+07 | -3.832389e+07 | -3.635526e+07 | -2.740098e+07 | . g3 | Linear Model | 10000.0 | 3.392920e+06 | 2.659435e+06 | -6.464701e+06 | 1.610506e+06 | 3.400628e+06 | 5.209953e+06 | 1.514587e+07 | . Random Choice | 10000.0 | -1.512092e+07 | 2.801968e+06 | -2.551133e+07 | -1.700093e+07 | -1.511649e+07 | -1.323328e+07 | -4.760673e+06 | . ( concat_df.groupby([&quot;Region&quot;, &quot;type&quot;]) .median() .rename({&quot;Profit (USD)&quot;: &quot;Expected Profit (USD)&quot;}, axis=1) ) . Expected Profit (USD) . Region type . g1 | Linear Model | 3.920459e+06 | . Random Choice | -1.696407e+07 | . g2 | Linear Model | 4.142425e+06 | . Random Choice | -3.832389e+07 | . g3 | Linear Model | 3.400628e+06 | . Random Choice | -1.511649e+07 | . Risks . ( concat_df.groupby([&quot;Region&quot;, &quot;type&quot;]) .apply(lambda x: (x &lt; 0).mean() * 100) .rename({&quot;Profit (USD)&quot;: &quot;Projected Chance of Loss&quot;}, axis=1) ) . Projected Chance of Loss . Region type . g1 | Linear Model | 6.75 | . Random Choice | 100.00 | . g2 | Linear Model | 2.21 | . Random Choice | 100.00 | . g3 | Linear Model | 10.01 | . Random Choice | 100.00 | . The g2 region has the lowest risk of loss at 2.21%, as well as the highest expected profit. .",
            "url": "https://jordwil.github.io/ds_blog/python/statistics/machine%20learning/2020/06/01/regression-with-bootstrapping.html",
            "relUrl": "/python/statistics/machine%20learning/2020/06/01/regression-with-bootstrapping.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA and Hypothesis Testing",
            "content": "Introduction . You work for the online store Ice, which sells videogames all over the world. User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. . You need to identify patterns that determine whether a game succeeds or not. This allows you to put your money on a potentially hot new item and plan advertising campaigns. . In front of you is data going back to 2016. Let’s imagine that it’s December 2016 and you’re planning a campaign for 2017. . The important thing is to get experience working with data. It doesn&#39;t really matter whether you&#39;re forecasting 2017 sales based on data from 2016 or 2027 sales based on data from 2026. . The data set contains the abbreviation ESRB (Entertainment Software Rating Board). The ESRB evaluates a game&#39;s content and assigns an appropriate age categories, such as Teen and Mature. . #collapse import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats from pathlib import Path import missingno as mn sns.set() . . f_path = Path(&quot;../datasets/games.csv&quot;) df = pd.read_csv(f_path) df.head() . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . Data Preparation . String Prep . def lower_cols(df): df.columns = df.columns.str.lower() return df . df = lower_cols(df) . Date Prep . pd.to_datetime(df[&quot;year_of_release&quot;], format=&quot;%Y&quot;) . 0 2006-01-01 1 1985-01-01 2 2008-01-01 3 2009-01-01 4 1996-01-01 ... 16710 2016-01-01 16711 2006-01-01 16712 2016-01-01 16713 2003-01-01 16714 2016-01-01 Name: year_of_release, Length: 16715, dtype: datetime64[ns] . Numerical Prep . There are no missing values for sales figures. . df.loc[:, df.columns.str.endswith(&quot;sales&quot;)].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 na_sales 16715 non-null float64 1 eu_sales 16715 non-null float64 2 jp_sales 16715 non-null float64 3 other_sales 16715 non-null float64 dtypes: float64(4) memory usage: 522.5 KB . There are many more user scores compared to critic scores. . df.loc[:, df.columns.str.endswith(&quot;score&quot;)].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 critic_score 8137 non-null float64 1 user_score 10014 non-null object dtypes: float64(1), object(1) memory usage: 261.3+ KB . df.loc[:, df.columns.str.endswith(&quot;score&quot;)] . critic_score user_score . 0 76.0 | 8 | . 1 NaN | NaN | . 2 82.0 | 8.3 | . 3 80.0 | 8 | . 4 NaN | NaN | . ... ... | ... | . 16710 NaN | NaN | . 16711 NaN | NaN | . 16712 NaN | NaN | . 16713 NaN | NaN | . 16714 NaN | NaN | . 16715 rows × 2 columns . Missing Values . There&#39;s a tbd value that we can use a NaN value in its place . df[&quot;user_score&quot;].value_counts() . tbd 2424 7.8 324 8 290 8.2 282 8.3 254 ... 0.5 2 9.6 2 0.6 2 0 1 9.7 1 Name: user_score, Length: 96, dtype: int64 . df.loc[df[&quot;user_score&quot;] == &quot;tbd&quot;, &quot;user_score&quot;] = None df[&quot;user_score&quot;] = df[&quot;user_score&quot;].astype(float) . There are multiple ratings for certain games. Let&#39;s examine these. . Nothing duplicated . (df.duplicated()).sum() . 0 . dup_name = df[&quot;name&quot;].value_counts() &gt; 1 . df[df[&quot;name&quot;].map(dup_name).astype(&quot;bool&quot;)].sort_values(&quot;name&quot;) . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating . 3862 Frozen: Olaf&#39;s Quest | DS | 2013.0 | Platform | 0.21 | 0.26 | 0.00 | 0.04 | NaN | NaN | NaN | . 3358 Frozen: Olaf&#39;s Quest | 3DS | 2013.0 | Platform | 0.27 | 0.27 | 0.00 | 0.05 | NaN | NaN | NaN | . 14658 007: Quantum of Solace | PC | 2008.0 | Action | 0.01 | 0.01 | 0.00 | 0.00 | 70.0 | 6.3 | T | . 9507 007: Quantum of Solace | DS | 2008.0 | Action | 0.11 | 0.01 | 0.00 | 0.01 | 65.0 | NaN | T | . 3120 007: Quantum of Solace | Wii | 2008.0 | Action | 0.29 | 0.28 | 0.01 | 0.07 | 54.0 | 7.5 | T | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 12648 pro evolution soccer 2011 | PC | 2010.0 | Sports | 0.00 | 0.05 | 0.00 | 0.01 | 79.0 | NaN | NaN | . 15612 uDraw Studio: Instant Artist | X360 | 2011.0 | Misc | 0.01 | 0.01 | 0.00 | 0.00 | 54.0 | 5.7 | E | . 8280 uDraw Studio: Instant Artist | Wii | 2011.0 | Misc | 0.06 | 0.09 | 0.00 | 0.02 | NaN | NaN | E | . 659 NaN | GEN | 1993.0 | NaN | 1.78 | 0.53 | 0.00 | 0.08 | NaN | NaN | NaN | . 14244 NaN | GEN | 1993.0 | NaN | 0.00 | 0.00 | 0.03 | 0.00 | NaN | NaN | NaN | . 7961 rows × 11 columns . Scores and rating values seem to be very sparse, and may not be very helpful for deeper analysis. . Note the large increase in missing values for the oldest games in the catalog. . #collapse mn.matrix(df.sort_values(&quot;year_of_release&quot;, ascending=False)) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c132a00&gt; . There&#39;s also a strong correlation between user score, critic score, and rating. . #collapse mn.heatmap(df) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11cbfb160&gt; . There are two values with no name. Let&#39;s drop these. . (df[&quot;name&quot;].isna()).sum() . 2 . df = df.loc[~df[&quot;name&quot;].isna()] . df[&quot;rating&quot;].value_counts() . E 3990 T 2961 M 1563 E10+ 1420 EC 8 K-A 3 RP 3 AO 1 Name: rating, dtype: int64 . It&#39;s possible that no ratings were found based on the data collection method used for some of the games. . Here&#39;s how to create total sales for each game. . df.loc[:, &quot;total_sales&quot;] = (df.loc[:, df.columns.str.endswith(&quot;sales&quot;)]).sum(axis=1) . df[&quot;year_of_release&quot;].describe() . count 16444.000000 mean 2006.486256 std 5.875525 min 1980.000000 25% 2003.000000 50% 2007.000000 75% 2010.000000 max 2016.000000 Name: year_of_release, dtype: float64 . #collapse sns.kdeplot(df[&quot;year_of_release&quot;]) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c818730&gt; . The popularity of a platform can vary wildly. . Before the 2000&#39;s, there were fewer platforms available, and made much more money some years than platforms available in later years. . I&#39;m going to use 2010 data to the newest data available as my set that may help represent a more modern trend in sales. . Using values starting at 2010 will provide us with around 25% of the data and will give us a better look at the most modern platforms. . #collapse sns.boxplot(df[&quot;year_of_release&quot;]) plt.show() . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x10d66d430&gt; . df = df[df[&quot;year_of_release&quot;] &gt;= 2010] . Let&#39;s look at the distribution now: . #collapse sns.boxplot(df[&quot;year_of_release&quot;]) plt.show() . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c1c5df0&gt; . median_plat_year_df = df.groupby([&quot;year_of_release&quot;, &quot;platform&quot;])[ &quot;total_sales&quot; ].median() . market_share_platform = ( median_plat_year_df.groupby(&quot;year_of_release&quot;) .apply(lambda x: (x / x.sum()) * 100) .rename(&quot;market_share&quot;) .reset_index() ) . It looks like for platforms that have had a market share of 20% or more at some point, have on average is 3.2 years . #collapse successful_plat_years = ( market_share_platform.query(&quot;market_share &gt;= 20&quot;) .groupby(&quot;platform&quot;) .size() .sort_values(ascending=False) ) sns.kdeplot(successful_plat_years.values) plt.xlim(0) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Probability Density&quot;) plt.title(&quot;Number of years platform has acheived more than 20% market share&quot;) plt.show() . . successful_plat_years.median() . 2.0 . On average, modern platforms hold a market share of 20% for 2 years . With this information, we can target platforms that have attained a market share of more than 20% for the first time in 2016, and consider those platforms having a high potential to maintain that market share next year. . #collapse sns.lineplot( data=market_share_platform, x=&quot;year_of_release&quot;, y=&quot;market_share&quot;, hue=&quot;platform&quot; ) plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0) plt.show() . . &lt;matplotlib.legend.Legend at 0x11c81c400&gt; . market_share_platform.query(&quot;market_share &gt;= 20&quot;) . year_of_release platform market_share . 3 2010.0 | PS3 | 31.050228 | . 6 2010.0 | X360 | 25.570776 | . 11 2011.0 | PS3 | 20.168067 | . 24 2012.0 | X360 | 25.144509 | . 29 2013.0 | PS4 | 38.012422 | . 39 2014.0 | PS4 | 27.334852 | . 45 2014.0 | XOne | 20.045558 | . 55 2015.0 | XOne | 20.675105 | . 61 2016.0 | Wii | 21.176471 | . 62 2016.0 | WiiU | 25.294118 | . Both the Wii and WiiU have yet to experience a second high market share event. I would consider investing in these platforms for 2017. . df[&quot;year_of_release&quot;] = pd.to_datetime(df[&quot;year_of_release&quot;], format=&quot;%Y&quot;) . g = ( df.groupby([&quot;year_of_release&quot;, &quot;platform&quot;])[&quot;total_sales&quot;] .median() .reset_index(&quot;platform&quot;) ) . g.pivot(columns=&quot;platform&quot;, values=&quot;total_sales&quot;) . platform 3DS DS PC PS2 PS3 PS4 PSP PSV Wii WiiU X360 XOne . year_of_release . 2010-01-01 NaN | 0.11 | 0.060 | 0.055 | 0.340 | NaN | 0.060 | NaN | 0.190 | NaN | 0.280 | NaN | . 2011-01-01 0.145 | 0.08 | 0.080 | 0.060 | 0.240 | NaN | 0.060 | 0.130 | 0.170 | NaN | 0.225 | NaN | . 2012-01-01 0.190 | 0.03 | 0.120 | NaN | 0.305 | NaN | 0.040 | 0.190 | 0.190 | 0.230 | 0.435 | NaN | . 2013-01-01 0.100 | 0.15 | 0.175 | NaN | 0.310 | 1.530 | 0.025 | 0.100 | 0.185 | 0.220 | 0.430 | 0.800 | . 2014-01-01 0.090 | NaN | 0.100 | NaN | 0.160 | 0.600 | 0.010 | 0.065 | 0.370 | 0.130 | 0.230 | 0.440 | . 2015-01-01 0.090 | NaN | 0.080 | NaN | 0.050 | 0.180 | 0.020 | 0.030 | 0.090 | 0.220 | 0.180 | 0.245 | . 2016-01-01 0.080 | NaN | 0.035 | NaN | 0.065 | 0.085 | NaN | 0.030 | 0.180 | 0.215 | 0.100 | 0.060 | . It looks like sales have stopped for certain systems; likely because a next generation platform overtook it. Certain platforms won&#39;t help us understand future sales like DS and PS2 . Furthermore, looking at our plots, it looks as if the Wii and Wiiu are experiencing very good sales in 2016. Futhermore, it looks like PS4 could experience a revival after a steep decline the previous year. . #collapse g.pivot(columns=&quot;platform&quot;, values=&quot;total_sales&quot;).drop([&quot;DS&quot;, &quot;PS2&quot;], axis=1).plot( figsize=(10, 10), sharex=True, sharey=True, logy=True ) plt.title(&quot;Total Sales by platform from 2010 to 2016&quot;) plt.ylabel(&quot;Total Sales USD Millions&quot;) plt.show() . . Looking at platform that have been sucessful over the past 6 years, we can see that the X360, PS3, XOne, WiiU, Wii, and PS4 are the top contenders. . #collapse col_order = ( df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;platform&quot;, y=&quot;total_sales&quot;, order=col_order) plt.yscale(&quot;log&quot;) plt.show() . . It looks like critic and user score has a strong correleation. . And both log total sales and scores have a positive correlation, though critic scores seem to have a stronger relationship. . #collapse df[&quot;log_total_sales&quot;] = np.log(df[&quot;total_sales&quot;]) sns.pairplot( data=df[[&quot;log_total_sales&quot;, &quot;critic_score&quot;, &quot;user_score&quot;]], corner=True, kind=&quot;reg&quot; ) plt.show() . . #collapse mult_plat = df[&quot;name&quot;].duplicated(keep=False) mp_df = df.query(&quot;@mult_plat&quot;) col_order = ( mp_df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;platform&quot;, y=&quot;total_sales&quot;, order=col_order) plt.yscale(&quot;log&quot;) plt.ylabel(&quot;Total Sales in Millions USD&quot;) plt.show() . . #collapse col_order = ( mp_df.groupby(&quot;genre&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;genre&quot;, y=&quot;total_sales&quot;, order=col_order) plt.xticks(rotation=45) plt.yscale(&quot;log&quot;) plt.ylabel(&quot;Total Sales in Millions USD&quot;) plt.show() . . Shooters seem to be the most profitable over the span we have selected. . df.columns . Index([&#39;name&#39;, &#39;platform&#39;, &#39;year_of_release&#39;, &#39;genre&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;, &#39;critic_score&#39;, &#39;user_score&#39;, &#39;rating&#39;, &#39;total_sales&#39;, &#39;log_total_sales&#39;], dtype=&#39;object&#39;) . User profile for each region . regions = [&quot;na_sales&quot;, &quot;eu_sales&quot;, &quot;jp_sales&quot;] reg_sale_df = pd.melt( df, [&quot;name&quot;, &quot;platform&quot;, &quot;genre&quot;, &quot;rating&quot;], regions, &quot;region&quot;, &quot;sales&quot; ) . reg_sale_df.groupby([&quot;region&quot;, &quot;platform&quot;])[&quot;sales&quot;].median() . region platform eu_sales 3DS 0.00 DS 0.00 PC 0.05 PS2 0.00 PS3 0.05 PS4 0.08 PSP 0.00 PSV 0.00 Wii 0.02 WiiU 0.07 X360 0.08 XOne 0.07 jp_sales 3DS 0.05 DS 0.00 PC 0.00 PS2 0.01 PS3 0.02 PS4 0.01 PSP 0.03 PSV 0.03 Wii 0.00 WiiU 0.00 X360 0.00 XOne 0.00 na_sales 3DS 0.01 DS 0.05 PC 0.00 PS2 0.00 PS3 0.09 PS4 0.06 PSP 0.00 PSV 0.00 Wii 0.11 WiiU 0.11 X360 0.16 XOne 0.12 Name: sales, dtype: float64 . reg_sale_df . name platform genre rating region sales . 0 Kinect Adventures! | X360 | Misc | E | na_sales | 15.00 | . 1 Grand Theft Auto V | PS3 | Action | M | na_sales | 7.02 | . 2 Grand Theft Auto V | X360 | Action | M | na_sales | 9.66 | . 3 Pokemon Black/Pokemon White | DS | Role-Playing | NaN | na_sales | 5.51 | . 4 Call of Duty: Modern Warfare 3 | X360 | Shooter | M | na_sales | 9.04 | . ... ... | ... | ... | ... | ... | ... | . 15826 Strawberry Nauts | PSV | Adventure | NaN | jp_sales | 0.01 | . 15827 Aiyoku no Eustia | PSV | Misc | NaN | jp_sales | 0.01 | . 15828 Samurai Warriors: Sanada Maru | PS3 | Action | NaN | jp_sales | 0.01 | . 15829 Haitaka no Psychedelica | PSV | Adventure | NaN | jp_sales | 0.01 | . 15830 Winning Post 8 2016 | PSV | Simulation | NaN | jp_sales | 0.01 | . 15831 rows × 6 columns . #collapse col_order = ( df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;platform&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . . North American and European Sales seem to follow similar trends in platform for the most part. Japanese sales strongly differ and are more focused on PSP, PSV and 3DS. PC and PS4 games seem to have higher sales in Europe. . #collapse col_order = ( df.groupby(&quot;genre&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;genre&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . . Most sales look pretty similar at first glance. There&#39;s a large difference in role playing games, where Japan has seen the most sales. . 41% of values for rating are missing, which makes imputation a challenge and will reduce the sample size that we&#39;re analyzing, that being said, lets look at how ratings affect sales per region. . #collapse col_order = ( df.groupby(&quot;rating&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;rating&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . . I suspect the rating systems aren&#39;t the same over all regions, so it may be difficult to make any conclusions with this dataset. . Hypothesis Testing . Average user ratings of the Xbox One and PC platforms are the same. . hypo = &quot;Average user ratings of the Xbox One and PC platforms are the same.&quot; pc_scores = df.loc[df[&quot;platform&quot;] == &quot;PC&quot;, &quot;user_score&quot;].dropna() xone_scores = df.loc[df[&quot;platform&quot;] == &quot;XOne&quot;, &quot;user_score&quot;].dropna() alpha = 0.05 result = stats.ttest_ind(pc_scores, xone_scores) if result.pvalue &lt; 0.05: print( f&quot;P value is close to {result.pvalue:.5f}. Reject the null hypothesis: n n{hypo}&quot; ) else: print( f&quot;P value is close to {result.pvalue:.5f}. Fail to reject the null hypothesis: n n{hypo}&quot; ) . P value is close to 0.98100. Fail to reject the null hypothesis: Average user ratings of the Xbox One and PC platforms are the same. . Average user ratings for the Action and Sports genres are different. . hypo = &quot;Average user ratings for the Action and Sports genres are different.&quot; action_scores = df.loc[df[&quot;genre&quot;] == &quot;Action&quot;, &quot;user_score&quot;].dropna() sports_scores = df.loc[df[&quot;genre&quot;] == &quot;Sports&quot;, &quot;user_score&quot;].dropna() alpha = 0.05 result = stats.ttest_ind(action_scores, sports_scores) if result.pvalue &lt; 0.05: print( f&quot;P value is close to {result.pvalue:.5f}. Reject the null hypothesis: n{hypo}&quot; ) else: print( f&quot;P value is close to {result.pvalue:.5f}. Fail to reject the null hypothesis: n {hypo}&quot; ) . P value is close to 0.00000. Reject the null hypothesis: Average user ratings for the Action and Sports genres are different. . Conclusion . We do notice major differences in sales when stratifying between platforms, genre, and region. Here are the major take aways. . Shooters, Platformers, Sports, Fighting, and Racing are the top selling genres on average from our modern games. | Wii and WiiU may be poised to take on a large market share next year | Generally, Critic Scores are a good indicator on how well a game will sell | Action and Sports games very likely do not share a distribution for sales. Action games are better sellers generally in this timeframe. | Japanese sales don&#39;t share the same trends compared to North American and European Sales. | .",
            "url": "https://jordwil.github.io/ds_blog/python/statistics/eda/2020/04/28/integrated-project.html",
            "relUrl": "/python/statistics/eda/2020/04/28/integrated-project.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Research on apartment sales ads",
            "content": "Introduction . I&#39;ll be using an archive of sales ads for realty in St. Petersburg, Russia, and the surrounding areas collected over the past few years. I&#39;ll focus on determining the market value of real estate properties. . There are two different types of data available for every apartment for sale. The first type is a user’s input. The second type is received automatically based upon the map data. For example, the distance from the downtown area, airport, the nearest park or body of water. . EDA . #collapse import pandas as pd import matplotlib.pyplot as plt import numpy as np from pathlib import Path import missingno as msno from scipy import signal import seaborn as sns . . #collapse from pathlib import Path file = Path(&quot;../datasets/real_estate_data_eng.csv&quot;) file = Path(&quot;../../../ds_projects/datasets/real_estate_data_eng.csv&quot;) df = pd.read_csv(file, sep=&quot; t&quot;) df.head() . . total_images last_price total_area first_day_exposition rooms ceiling_height floors_total living_area floor is_apartment ... kitchen_area balcony locality_name airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . 0 20 | 13000000.0 | 108.0 | 2019-03-07T00:00:00 | 3 | 2.70 | 16.0 | 51.0 | 8 | NaN | ... | 25.0 | NaN | Saint Peterburg | 18863.0 | 16028.0 | 1.0 | 482.0 | 2.0 | 755.0 | NaN | . 1 7 | 3350000.0 | 40.4 | 2018-12-04T00:00:00 | 1 | NaN | 11.0 | 18.6 | 1 | NaN | ... | 11.0 | 2.0 | Shushary village | 12817.0 | 18603.0 | 0.0 | NaN | 0.0 | NaN | 81.0 | . 2 10 | 5196000.0 | 56.0 | 2015-08-20T00:00:00 | 2 | NaN | 5.0 | 34.3 | 4 | NaN | ... | 8.3 | 0.0 | Saint Peterburg | 21741.0 | 13933.0 | 1.0 | 90.0 | 2.0 | 574.0 | 558.0 | . 3 0 | 64900000.0 | 159.0 | 2015-07-24T00:00:00 | 3 | NaN | 14.0 | NaN | 9 | NaN | ... | NaN | 0.0 | Saint Peterburg | 28098.0 | 6800.0 | 2.0 | 84.0 | 3.0 | 234.0 | 424.0 | . 4 2 | 10000000.0 | 100.0 | 2018-06-19T00:00:00 | 2 | 3.03 | 14.0 | 32.0 | 13 | NaN | ... | 41.0 | NaN | Saint Peterburg | 31856.0 | 8098.0 | 2.0 | 112.0 | 1.0 | 48.0 | 121.0 | . 5 rows × 22 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23699 entries, 0 to 23698 Data columns (total 22 columns): # Column Non-Null Count Dtype -- -- 0 total_images 23699 non-null int64 1 last_price 23699 non-null float64 2 total_area 23699 non-null float64 3 first_day_exposition 23699 non-null object 4 rooms 23699 non-null int64 5 ceiling_height 14504 non-null float64 6 floors_total 23613 non-null float64 7 living_area 21796 non-null float64 8 floor 23699 non-null int64 9 is_apartment 2775 non-null object 10 studio 23699 non-null bool 11 open_plan 23699 non-null bool 12 kitchen_area 21421 non-null float64 13 balcony 12180 non-null float64 14 locality_name 23650 non-null object 15 airports_nearest 18157 non-null float64 16 cityCenters_nearest 18180 non-null float64 17 parks_around3000 18181 non-null float64 18 parks_nearest 8079 non-null float64 19 ponds_around3000 18181 non-null float64 20 ponds_nearest 9110 non-null float64 21 days_exposition 20518 non-null float64 dtypes: bool(2), float64(14), int64(3), object(3) memory usage: 3.7+ MB . The proportion of values that are missing in this dataset . #collapse ((df.isna().mean() * 100).round(2).rename(&quot;Percent Missing&quot;).sort_values()) . . total_images 0.00 open_plan 0.00 floor 0.00 rooms 0.00 studio 0.00 total_area 0.00 last_price 0.00 first_day_exposition 0.00 locality_name 0.21 floors_total 0.36 living_area 8.03 kitchen_area 9.61 days_exposition 13.42 ponds_around3000 23.28 parks_around3000 23.28 cityCenters_nearest 23.29 airports_nearest 23.38 ceiling_height 38.80 balcony 48.61 ponds_nearest 61.56 parks_nearest 65.91 is_apartment 88.29 Name: Percent Missing, dtype: float64 . When parks nearest is missing values, parks around 3000 is zero. . df[[&quot;parks_around3000&quot;, &quot;parks_nearest&quot;]].query(&quot;parks_nearest.isnull()&quot;).groupby( &quot;parks_around3000&quot; ).size() . parks_around3000 0.0 10102 dtype: int64 . Only zero values when parks around 3000 is null value. I&#39;ll reassign parks_nearest to 0 . #collapse msno.matrix(df) plt.title(&quot;Missing Values Locations&quot;); . . #collapse msno.heatmap(df) plt.title(&quot;Missing Value Correlations&quot;); . . df.describe() . total_images last_price total_area rooms ceiling_height floors_total living_area floor kitchen_area balcony airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . count 23699.000000 | 2.369900e+04 | 23699.000000 | 23699.000000 | 14504.000000 | 23613.000000 | 21796.000000 | 23699.000000 | 21421.000000 | 12180.000000 | 18157.000000 | 18180.000000 | 18181.000000 | 8079.000000 | 18181.000000 | 9110.000000 | 20518.000000 | . mean 9.858475 | 6.541549e+06 | 60.348651 | 2.070636 | 2.771499 | 10.673824 | 34.457852 | 5.892358 | 10.569807 | 1.150082 | 28793.672193 | 14191.277833 | 0.611408 | 490.804555 | 0.770255 | 517.980900 | 180.888634 | . std 5.682529 | 1.088701e+07 | 35.654083 | 1.078405 | 1.261056 | 6.597173 | 22.030445 | 4.885249 | 5.905438 | 1.071300 | 12630.880622 | 8608.386210 | 0.802074 | 342.317995 | 0.938346 | 277.720643 | 219.727988 | . min 0.000000 | 1.219000e+04 | 12.000000 | 0.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.300000 | 0.000000 | 0.000000 | 181.000000 | 0.000000 | 1.000000 | 0.000000 | 13.000000 | 1.000000 | . 25% 6.000000 | 3.400000e+06 | 40.000000 | 1.000000 | 2.520000 | 5.000000 | 18.600000 | 2.000000 | 7.000000 | 0.000000 | 18585.000000 | 9238.000000 | 0.000000 | 288.000000 | 0.000000 | 294.000000 | 45.000000 | . 50% 9.000000 | 4.650000e+06 | 52.000000 | 2.000000 | 2.650000 | 9.000000 | 30.000000 | 4.000000 | 9.100000 | 1.000000 | 26726.000000 | 13098.500000 | 0.000000 | 455.000000 | 1.000000 | 502.000000 | 95.000000 | . 75% 14.000000 | 6.800000e+06 | 69.900000 | 3.000000 | 2.800000 | 16.000000 | 42.300000 | 8.000000 | 12.000000 | 2.000000 | 37273.000000 | 16293.000000 | 1.000000 | 612.000000 | 1.000000 | 729.000000 | 232.000000 | . max 50.000000 | 7.630000e+08 | 900.000000 | 19.000000 | 100.000000 | 60.000000 | 409.700000 | 33.000000 | 112.000000 | 5.000000 | 84869.000000 | 65968.000000 | 3.000000 | 3190.000000 | 3.000000 | 1344.000000 | 1580.000000 | . Conclusion . The data file is tab separated in defiance of it&#39;s extention of csv. | A large proportion of is_apartment column missing. | . Data preprocessing . Missing Value Analysis . is_aparment . Around 88% of this data is missing, and it&#39;s unclear what that value may be, as both boolean values exist within the data. . #collapse df[&quot;is_apartment&quot;].dropna().astype(str).hist() plt.title(&quot;Is Apartment value counts&quot;); . . I&#39;ve decided to drop this feature from my analysis, as i&#39;m not confident that imputation can &#39;rescue&#39; the distribution of this sample. . df = df.drop(&quot;is_apartment&quot;, axis=1) . parks_nearest and ponds_nearest . parks and ponds nearest columns contain a large proportion of missing data as well, with around 66% and 62% respectively. . #collapse df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;box&quot;) plt.title(&quot;Distance of nearest parks and ponds&quot;) plt.ylabel(&quot;Distance (km)&quot;) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;hist&quot;, subplots=True, bins=50); plt.xlabel(&quot;Distance (km)&quot;); . . Interestingly, there are many outliers in the parks_nearest column, and that trend isn&#39;t present for ponds nearest both contain a long tail where a median imputation would be most appropriate in an attempt to represent the distribution of this sample. What does this distribution look like after imputation? . #collapse park_pond_imputation = dict(df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].median()) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]] = df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].fillna( park_pond_imputation ) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;hist&quot;, subplots=True, bins=50); . . It seems strange to impute such a large proportion of the data. We will leave it in for now. . balcony, floors_total, living_area, kitchen_area, airports_nearest, cityCenters_nearest, parks_around3000, ponds_around3000, days_exposition, ceiling_height . Due to the rough poisson distribtion, the tails are going to be uneven, and a median may be the best measure of central tendancy. . #collapse df[&quot;balcony&quot;].fillna(df[&quot;balcony&quot;].median(), inplace=True) df[&quot;balcony&quot;].hist() plt.ylabel(&quot;Frequency&quot;) plt.xlabel(&quot;Number of balconies&quot;); . . #collapse df[&quot;floors_total&quot;].fillna(df[&quot;floors_total&quot;].median(), inplace=True) df[&quot;floors_total&quot;].hist(bins=25) plt.xlabel(&quot;Number of floors&quot;) plt.ylabel(&quot;Frequency&quot;); . . Mode seems to be the most appropriate measure to fill the missing values in balcony. . #collapse df[&quot;living_area&quot;].fillna(df[&quot;living_area&quot;].median(), inplace=True) df[&quot;living_area&quot;].hist(bins=50) plt.xlabel(&quot;Living Area Square Feet&quot;) plt.ylabel(&quot;Frequency&quot;); . . #collapse df[&quot;kitchen_area&quot;].hist(bins=50) plt.xlabel(&quot;Kitchen Area Square Feet&quot;) plt.ylabel(&quot;Frequency&quot;); . . #collapse df[&quot;airports_nearest&quot;].fillna(df[&quot;airports_nearest&quot;].median(), inplace=True) df[&quot;airports_nearest&quot;].hist(bins=50) plt.ylabel(&quot;Frequency&quot;) plt.xlabel(&quot;Nearest airport in kilometers&quot;); . . #collapse df[&quot;cityCenters_nearest&quot;].fillna(df[&quot;cityCenters_nearest&quot;].median(), inplace=True) df[&quot;cityCenters_nearest&quot;].hist(bins=36) plt.ylabel(&quot;Frequency&quot;) plt.xlabel(&quot;Nearest city center in kilometers&quot;); . . #collapse df[&quot;parks_around3000&quot;].fillna(df[&quot;parks_around3000&quot;].median(), inplace=True) df[&quot;parks_around3000&quot;].hist() plt.xlabel(&quot;Number of parks in the radius of 3000 kilometers&quot;) plt.ylabel(&quot;Frequency&quot;); . . #collapse df[&quot;ponds_around3000&quot;].fillna(df[&quot;ponds_around3000&quot;].median(), inplace=True) df[&quot;ponds_around3000&quot;].hist(); . . #collapse df[&quot;days_exposition&quot;].fillna(df[&quot;days_exposition&quot;].median(), inplace=True) df[&quot;days_exposition&quot;].hist(bins=50); . . #collapse df[&quot;ceiling_height&quot;].plot(kind=&quot;box&quot;); . . Somone has quite the ceiling! . #collapse df[&quot;ceiling_height&quot;].fillna(df[&quot;ceiling_height&quot;].median(), inplace=True) df[&quot;ceiling_height&quot;].hist(bins=40, range=(0, 20)); . . Locality name . Let&#39;s fill in NA values with &#39;Unknown&#39; string. . #collapse df[&quot;locality_name&quot;].fillna(&quot;Unknown&quot;, inplace=True) df[&quot;locality_name&quot;].value_counts().head(10).plot(kind=&quot;bar&quot;); . . Let&#39;s make sure we took care of all na values . df.isna().sum() . total_images 0 last_price 0 total_area 0 first_day_exposition 0 rooms 0 ceiling_height 0 floors_total 0 living_area 0 floor 0 studio 0 open_plan 0 kitchen_area 0 balcony 0 locality_name 0 airports_nearest 0 cityCenters_nearest 0 parks_around3000 0 parks_nearest 0 ponds_around3000 0 ponds_nearest 0 days_exposition 0 dtype: int64 . We can convert first_day_exposition to a datetime dtype . df[&quot;first_day_exposition&quot;] = pd.to_datetime( df[&quot;first_day_exposition&quot;], format=&quot;%Y-%m-%dT%H:%M:%S&quot; ) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23699 entries, 0 to 23698 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 total_images 23699 non-null int64 1 last_price 23699 non-null float64 2 total_area 23699 non-null float64 3 first_day_exposition 23699 non-null datetime64[ns] 4 rooms 23699 non-null int64 5 ceiling_height 23699 non-null float64 6 floors_total 23699 non-null float64 7 living_area 23699 non-null float64 8 floor 23699 non-null int64 9 studio 23699 non-null bool 10 open_plan 23699 non-null bool 11 kitchen_area 23699 non-null float64 12 balcony 23699 non-null float64 13 locality_name 23699 non-null object 14 airports_nearest 23699 non-null float64 15 cityCenters_nearest 23699 non-null float64 16 parks_around3000 23699 non-null float64 17 parks_nearest 23699 non-null float64 18 ponds_around3000 23699 non-null float64 19 ponds_nearest 23699 non-null float64 20 days_exposition 23699 non-null float64 dtypes: bool(2), datetime64[ns](1), float64(14), int64(3), object(1) memory usage: 3.5+ MB . Type conversions . Which float values should be int values? . We can determine this by looking for columns that can be represented by an integer type without losing information. . decimal_vals = ((df.loc[:, df.dtypes == float] % 1) != 0).sum().sort_values() replace_types = decimal_vals.map({0: &quot;int64&quot;}).dropna() decimal_vals . last_price 0 floors_total 0 balcony 0 airports_nearest 0 parks_around3000 0 parks_nearest 0 ponds_around3000 0 ponds_nearest 0 days_exposition 3997 cityCenters_nearest 5519 living_area 10978 total_area 11249 kitchen_area 13776 ceiling_height 22509 dtype: int64 . Based on the data from the code book, days_exposition should be represented as an integer. . decimal_vals.drop(&quot;days_exposition&quot;, inplace=True) . df = df.astype(replace_types.to_dict()).assign( days_exposition=df[&quot;days_exposition&quot;].round().astype(int) ) df.head() . total_images last_price total_area first_day_exposition rooms ceiling_height floors_total living_area floor studio ... kitchen_area balcony locality_name airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . 0 20 | 13000000 | 108.0 | 2019-03-07 | 3 | 2.70 | 16 | 51.0 | 8 | False | ... | 25.0 | 1 | Saint Peterburg | 18863 | 16028.0 | 1 | 482 | 2 | 755 | 95 | . 1 7 | 3350000 | 40.4 | 2018-12-04 | 1 | 2.65 | 11 | 18.6 | 1 | False | ... | 11.0 | 2 | Shushary village | 12817 | 18603.0 | 0 | 455 | 0 | 502 | 81 | . 2 10 | 5196000 | 56.0 | 2015-08-20 | 2 | 2.65 | 5 | 34.3 | 4 | False | ... | 8.3 | 0 | Saint Peterburg | 21741 | 13933.0 | 1 | 90 | 2 | 574 | 558 | . 3 0 | 64900000 | 159.0 | 2015-07-24 | 3 | 2.65 | 14 | 30.0 | 9 | False | ... | 9.1 | 0 | Saint Peterburg | 28098 | 6800.0 | 2 | 84 | 3 | 234 | 424 | . 4 2 | 10000000 | 100.0 | 2018-06-19 | 2 | 3.03 | 14 | 32.0 | 13 | False | ... | 41.0 | 1 | Saint Peterburg | 31856 | 8098.0 | 2 | 112 | 1 | 48 | 121 | . 5 rows × 21 columns . Transformations . Assign: Price per square meter . df[&quot;price_per_sq_meter&quot;] = df[&quot;last_price&quot;] / df[&quot;total_area&quot;] . Assign: day of week, month, and year. . df[&quot;day_of_week&quot;] = df[&quot;first_day_exposition&quot;].dt.dayofweek df[&quot;month&quot;] = df[&quot;first_day_exposition&quot;].dt.month df[&quot;year&quot;] = df[&quot;first_day_exposition&quot;].dt.year . Assign : floor type (top, bottom, other) . def gen_floor_type(x): if x[&quot;floors_total&quot;] == 1: return &quot;bottom&quot; elif x[&quot;floors_total&quot;] == x[&quot;floor&quot;]: return &quot;top&quot; else: return &quot;other&quot; df = df.assign(floor_type=df.apply(gen_floor_type, axis=1)) . Assign: living space to total area ratio, kitchen space to total area ratio . df[&quot;living_space_ratio&quot;] = df[&quot;living_area&quot;] / df[&quot;total_area&quot;] df[&quot;kitchen_space_ratio&quot;] = df[&quot;kitchen_area&quot;] / df[&quot;total_area&quot;] . EDA . Examining total_area, last_price, rooms, and ceiling_height. . Plotting a histogram is a great way to understand the distribution of a feature. . We will try to use these features to help understand how price relates to total area, number of rooms, and ceiling height. . #collapse cols_of_interest = [&quot;total_area&quot;, &quot;last_price&quot;, &quot;rooms&quot;, &quot;ceiling_height&quot;] pd.plotting.scatter_matrix(df[cols_of_interest], figsize=(10, 10)); . . df[cols_of_interest].describe() . total_area last_price rooms ceiling_height . count 23699.000000 | 2.369900e+04 | 23699.000000 | 23699.000000 | . mean 60.348651 | 6.541549e+06 | 2.070636 | 2.724358 | . std 35.654083 | 1.088701e+07 | 1.078405 | 0.988298 | . min 12.000000 | 1.219000e+04 | 0.000000 | 1.000000 | . 25% 40.000000 | 3.400000e+06 | 1.000000 | 2.600000 | . 50% 52.000000 | 4.650000e+06 | 2.000000 | 2.650000 | . 75% 69.900000 | 6.800000e+06 | 3.000000 | 2.700000 | . max 900.000000 | 7.630000e+08 | 19.000000 | 100.000000 | . Examining the numer of days it takes to sell an apartment . #collapse df[&quot;days_exposition&quot;].hist(bins=50) plt.title(&quot;Number of days of a property has been up for sale&quot;) plt.xlabel(&quot;Days&quot;) plt.ylabel(&quot;Frequency&quot;); . . df[&quot;days_exposition&quot;].describe() . count 23699.000000 mean 169.360226 std 206.535633 min 1.000000 25% 45.000000 50% 95.000000 75% 199.000000 max 1580.000000 Name: days_exposition, dtype: float64 . #collapse df[&quot;days_exposition&quot;].plot(kind=&quot;box&quot;); . . The average time it takes to complete a sale is around 95 days. The interquartile range allows us to determine what are considered unusally quick or long sales. Anything less than 45 days or more than 199 days is outside the interquartile range. . Removal of rare and outlying values. . I&#39;ll remove outlying values for our columns of interest in order to have a better understanding of how these features will affect the price. . Density of names is quite small for most here. I&#39;ll drop the 250 rarest localities. . def get_iqr(series): iqr = series.quantile(0.75) - series.quantile(0.25) return iqr def get_wiskers(series): low_out = series.quantile(0.25) - (1.5 * get_iqr(series)) high_out = series.quantile(0.75) + (1.5 * get_iqr(series)) return low_out, high_out def mask_outliers(series): return series.where(series.between(*get_wiskers(series))) . Let&#39;s test this function: . df[cols_of_interest].apply(get_wiskers) . total_area (-4.8500000000000085, 114.75000000000001) last_price (-1700000.0, 11900000.0) rooms (-2.0, 6.0) ceiling_height (2.45, 2.8500000000000005) dtype: object . df[cols_of_interest].apply(mask_outliers) . total_area last_price rooms ceiling_height . 0 108.00 | NaN | 3.0 | 2.70 | . 1 40.40 | 3350000.0 | 1.0 | 2.65 | . 2 56.00 | 5196000.0 | 2.0 | 2.65 | . 3 NaN | NaN | 3.0 | 2.65 | . 4 100.00 | 10000000.0 | 2.0 | NaN | . ... ... | ... | ... | ... | . 23694 NaN | 9700000.0 | 3.0 | NaN | . 23695 59.00 | 3100000.0 | 3.0 | 2.65 | . 23696 56.70 | 2500000.0 | 2.0 | 2.65 | . 23697 76.75 | 11475000.0 | 2.0 | NaN | . 23698 32.30 | 1350000.0 | 1.0 | 2.50 | . 23699 rows × 4 columns . Looks good! . #collapse pd.plotting.scatter_matrix( df[cols_of_interest].apply(mask_outliers), figsize=(10, 10), alpha=0.1 ); . . The trends are much clearer here. There&#39;s a clear coreleation between last price and total area as well as number of rooms, with the strongest linear correlation coupled to total area. . #collapse sns.heatmap(df[cols_of_interest].apply(mask_outliers).corr(), annot=True); . . df[&quot;floor_type&quot;] . 0 other 1 other 2 other 3 other 4 other ... 23694 other 23695 other 23696 other 23697 other 23698 other Name: floor_type, Length: 23699, dtype: object . Let&#39;s add some more interesting features to our analysis. . df[[&quot;cityCenters_nearest&quot;, &quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]].describe() . cityCenters_nearest day_of_week month year . count 23699.000000 | 23699.000000 | 23699.000000 | 23699.000000 | . mean 13936.792797 | 2.569307 | 6.400101 | 2017.367146 | . std 7553.779633 | 1.789082 | 3.491561 | 1.039771 | . min 181.000000 | 0.000000 | 1.000000 | 2014.000000 | . 25% 10927.000000 | 1.000000 | 3.000000 | 2017.000000 | . 50% 13098.500000 | 3.000000 | 6.000000 | 2017.000000 | . 75% 15248.500000 | 4.000000 | 10.000000 | 2018.000000 | . max 65968.000000 | 6.000000 | 12.000000 | 2019.000000 | . Let&#39;s remove outliers from cityCenters_nearest as well . cols_of_interest = cols_of_interest + [&quot;cityCenters_nearest&quot;] . Remove outliers from analysis . #collapse pd.plotting.scatter_matrix( df[cols_of_interest] .apply(mask_outliers) .join(df[[&quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]]), figsize=(20, 20), alpha=0.2, ); . . #collapse plt.figure(figsize=(10, 10)) sns.heatmap( df[cols_of_interest] .apply(mask_outliers) .join(df[[&quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]]) .corr(), annot=True, ); . . #collapse df.set_index(&quot;first_day_exposition&quot;)[&quot;last_price&quot;].resample(&quot;y&quot;).median().plot() . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c33cf70&gt; . Theres a slight correlation with proximity to city centers and no clear linear relationship between time related values recorded. . top10_localities = df[&quot;locality_name&quot;].value_counts().iloc[:10].index locality_analysis = df.query(&quot;locality_name in @top10_localities&quot;).pivot_table( index=[&quot;locality_name&quot;], values=[&quot;price_per_sq_meter&quot;], aggfunc=[&quot;median&quot;, &quot;count&quot;] ) locality_analysis.columns = [&quot;median_price_per_sq_meter&quot;, &quot;num_occurences&quot;] . locality_analysis.sort_values(&quot;median_price_per_sq_meter&quot;, ascending=False).assign( median_price_per_sq_meter=locality_analysis[&quot;median_price_per_sq_meter&quot;].round(2) ) . median_price_per_sq_meter num_occurences . locality_name . Saint Peterburg 104761.90 | 15721 | . Pushkin 100000.00 | 369 | . Kudrovo village 91860.47 | 299 | . Pargolovo village 91642.86 | 327 | . Murino village 85878.46 | 556 | . Shushary village 76876.17 | 440 | . Kolpino 74723.75 | 338 | . Gatchina 67796.61 | 307 | . Vsevolozhsk 65789.47 | 398 | . Vyborg 58158.32 | 237 | . The highest median price per square meter is Saint Peterburg, and Vyborg being the most affordable. . #collapse df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[ [&quot;cityCenters_nearest&quot;, &quot;last_price&quot;] ].plot(kind=&quot;scatter&quot;, x=&quot;cityCenters_nearest&quot;, y=&quot;last_price&quot;); . . index_range = ( df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[&quot;cityCenters_nearest&quot;] .agg([&quot;min&quot;, &quot;max&quot;]) .add([0, 1]) .astype(int) .tolist() ) downtown_interp = ( df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[[&quot;cityCenters_nearest&quot;]] .join((df[&quot;last_price&quot;] / df[&quot;cityCenters_nearest&quot;]).rename(&quot;pr_per_nearest_km&quot;)) .round({&quot;cityCenters_nearest&quot;: 0}) .astype({&quot;cityCenters_nearest&quot;: int}) .pivot_table( index=&quot;cityCenters_nearest&quot;, values=&quot;pr_per_nearest_km&quot;, aggfunc=&quot;median&quot; ) .reindex(range(*index_range)) .interpolate(method=&quot;spline&quot;, order=2) .diff() .rolling(window=300, win_type=&quot;triang&quot;) .mean() ) . peak_x = signal.find_peaks(downtown_interp.values.flatten(), height=200)[0] . #collapse downtown_interp.plot(style=&quot;b&quot;, xlim=(0, 3000)) for peak in downtown_interp.iloc[peak_x].index.tolist(): plt.axvline(peak) . . There seem to be two areas where the price will sharply change. I&#39;m going to use 540 km as the downtown limit. . downtown_interp.iloc[peak_x] . pr_per_nearest_km . cityCenters_nearest . 540 211.074453 | . 574 453.286384 | . 1075 439.963977 | . 1088 412.275703 | . 1118 377.902838 | . 1164 298.819159 | . 1201 200.436884 | . #collapse downtown_limit = 574 city_center_df = df.query( &quot;cityCenters_nearest &lt; @downtown_limit &amp; locality_name == &#39;Saint Peterburg&#39;&quot; ) sns.heatmap( city_center_df[[&quot;total_area&quot;, &quot;last_price&quot;, &quot;rooms&quot;, &quot;ceiling_height&quot;]].corr(), annot=True, ); . . Rooms and price have a strong correlation between total area. . #collapse city_center_price_df = city_center_df[ [&quot;last_price&quot;, &quot;rooms&quot;, &quot;floor&quot;, &quot;cityCenters_nearest&quot;, &quot;first_day_exposition&quot;] ] . . sns.heatmap(city_center_price_df.corr(), annot=True); . #collapse city_center_price_df.set_index(&quot;first_day_exposition&quot;)[&quot;last_price&quot;].resample( &quot;y&quot; ).median().plot(); . . It seems like 2016 had prices that were very low, and steadily increased each year. . Step 5. Overall conclusion . There doesn&#39;t seem to be a dramatic change in overall trends when comparing the instances near St. Petersburg city center and the entire dataset. . It most dramatic trend seems to be how publication year and price trended. There seems to be a much stronger increase in median price per year in the St. Petersburg city center values from 2016 to 2019. .",
            "url": "https://jordwil.github.io/ds_blog/eda/jupyter/2020/02/20/eda-project.html",
            "relUrl": "/eda/jupyter/2020/02/20/eda-project.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a data scientist with a background in cell biology and genomics, and enjoy applying my background problems outside my field. When I’m not sciencing, I’m gardening, hiking, or biking. . If you’re an aspiring data scientist or analyst, please let me know in the comments if the posts are helpful and why. I’d really appreciate the feedback. . If you’re a grizzled veteran in the field and see something off or you’d approach the problem differently, I’d like to hear from you too. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jordwil.github.io/ds_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jordwil.github.io/ds_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}