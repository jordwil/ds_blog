{
  
    
        "post0": {
            "title": "Title",
            "content": "Integrated Project - 1 . Intro . You work for the online store Ice, which sells videogames all over the world. User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. . You need to identify patterns that determine whether a game succeeds or not. This allows you to put your money on a potentially hot new item and plan advertising campaigns. . In front of you is data going back to 2016. Let’s imagine that it’s December 2016 and you’re planning a campaign for 2017. . The important thing is to get experience working with data. It doesn&#39;t really matter whether you&#39;re forecasting 2017 sales based on data from 2016 or 2027 sales based on data from 2026. . The data set contains the abbreviation ESRB (Entertainment Software Rating Board). The ESRB evaluates a game&#39;s content and assigns an appropriate age categories, such as Teen and Mature. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats from pathlib import Path import missingno as mn sns.set() . f_path = Path(&quot;../datasets/games.csv&quot;) df = pd.read_csv(f_path) df.head() . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . Data Preparation . String Prep . def lower_cols(df): df.columns = df.columns.str.lower() return df . df = lower_cols(df) . Date Prep . pd.to_datetime(df[&quot;year_of_release&quot;], format=&quot;%Y&quot;) . 0 2006-01-01 1 1985-01-01 2 2008-01-01 3 2009-01-01 4 1996-01-01 ... 16710 2016-01-01 16711 2006-01-01 16712 2016-01-01 16713 2003-01-01 16714 2016-01-01 Name: year_of_release, Length: 16715, dtype: datetime64[ns] . Numerical Prep . There are no missing values for sales figures. . df.loc[:, df.columns.str.endswith(&quot;sales&quot;)].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 na_sales 16715 non-null float64 1 eu_sales 16715 non-null float64 2 jp_sales 16715 non-null float64 3 other_sales 16715 non-null float64 dtypes: float64(4) memory usage: 522.5 KB . There are many more user scores compared to critic scores. . df.loc[:, df.columns.str.endswith(&quot;score&quot;)].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 critic_score 8137 non-null float64 1 user_score 10014 non-null object dtypes: float64(1), object(1) memory usage: 261.3+ KB . df.loc[:, df.columns.str.endswith(&quot;score&quot;)] . critic_score user_score . 0 76.0 | 8 | . 1 NaN | NaN | . 2 82.0 | 8.3 | . 3 80.0 | 8 | . 4 NaN | NaN | . ... ... | ... | . 16710 NaN | NaN | . 16711 NaN | NaN | . 16712 NaN | NaN | . 16713 NaN | NaN | . 16714 NaN | NaN | . 16715 rows × 2 columns . Missing Values . There&#39;s a tbd value that we can use a NaN value in its place . df[&quot;user_score&quot;].value_counts() . tbd 2424 7.8 324 8 290 8.2 282 8.3 254 ... 0.5 2 9.6 2 0.6 2 0 1 9.7 1 Name: user_score, Length: 96, dtype: int64 . df.loc[df[&quot;user_score&quot;] == &quot;tbd&quot;, &quot;user_score&quot;] = None df[&quot;user_score&quot;] = df[&quot;user_score&quot;].astype(float) . There are multiple ratings for certain games. Let&#39;s examine these. . Nothing duplicated . (df.duplicated()).sum() . 0 . dup_name = df[&quot;name&quot;].value_counts() &gt; 1 . df[df[&quot;name&quot;].map(dup_name).astype(&quot;bool&quot;)].sort_values(&quot;name&quot;) . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating . 3862 Frozen: Olaf&#39;s Quest | DS | 2013.0 | Platform | 0.21 | 0.26 | 0.00 | 0.04 | NaN | NaN | NaN | . 3358 Frozen: Olaf&#39;s Quest | 3DS | 2013.0 | Platform | 0.27 | 0.27 | 0.00 | 0.05 | NaN | NaN | NaN | . 14658 007: Quantum of Solace | PC | 2008.0 | Action | 0.01 | 0.01 | 0.00 | 0.00 | 70.0 | 6.3 | T | . 9507 007: Quantum of Solace | DS | 2008.0 | Action | 0.11 | 0.01 | 0.00 | 0.01 | 65.0 | NaN | T | . 3120 007: Quantum of Solace | Wii | 2008.0 | Action | 0.29 | 0.28 | 0.01 | 0.07 | 54.0 | 7.5 | T | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 12648 pro evolution soccer 2011 | PC | 2010.0 | Sports | 0.00 | 0.05 | 0.00 | 0.01 | 79.0 | NaN | NaN | . 15612 uDraw Studio: Instant Artist | X360 | 2011.0 | Misc | 0.01 | 0.01 | 0.00 | 0.00 | 54.0 | 5.7 | E | . 8280 uDraw Studio: Instant Artist | Wii | 2011.0 | Misc | 0.06 | 0.09 | 0.00 | 0.02 | NaN | NaN | E | . 659 NaN | GEN | 1993.0 | NaN | 1.78 | 0.53 | 0.00 | 0.08 | NaN | NaN | NaN | . 14244 NaN | GEN | 1993.0 | NaN | 0.00 | 0.00 | 0.03 | 0.00 | NaN | NaN | NaN | . 7961 rows × 11 columns . Scores and rating values seem to be very sparse, and may not be very helpful for deeper analysis. . Note the large increase in missing values for the oldest games in the catalog. . mn.matrix(df.sort_values(&quot;year_of_release&quot;, ascending=False)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c132a00&gt; . There&#39;s also a strong correlation between user score, critic score, and rating. . mn.heatmap(df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11cbfb160&gt; . There are two values with no name. Let&#39;s drop these. . (df[&quot;name&quot;].isna()).sum() . 2 . df = df.loc[~df[&quot;name&quot;].isna()] . df[&quot;rating&quot;].value_counts() . E 3990 T 2961 M 1563 E10+ 1420 EC 8 K-A 3 RP 3 AO 1 Name: rating, dtype: int64 . It&#39;s possible that no ratings were found based on the data collection method used for some of the games. . Here&#39;s how to create total sales for each game. . df.loc[:, &quot;total_sales&quot;] = (df.loc[:, df.columns.str.endswith(&quot;sales&quot;)]).sum(axis=1) . df[&quot;year_of_release&quot;].describe() . count 16444.000000 mean 2006.486256 std 5.875525 min 1980.000000 25% 2003.000000 50% 2007.000000 75% 2010.000000 max 2016.000000 Name: year_of_release, dtype: float64 . sns.kdeplot(df[&quot;year_of_release&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c818730&gt; . The popularity of a platform can vary wildly. . Before the 2000&#39;s, there were fewer platforms available, and made much more money some years than platforms available in later years. . I&#39;m going to use 2010 data to the newest data available as my set that may help represent a more modern trend in sales. . Using values starting at 2010 will provide us with around 25% of the data and will give us a better look at the most modern platforms. . sns.boxplot(df[&quot;year_of_release&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x10d66d430&gt; . df = df[df[&quot;year_of_release&quot;] &gt;= 2010] . Let&#39;s look at the distribution now: . sns.boxplot(df[&quot;year_of_release&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11c1c5df0&gt; . median_plat_year_df = df.groupby([&quot;year_of_release&quot;, &quot;platform&quot;])[ &quot;total_sales&quot; ].median() . market_share_platform = ( median_plat_year_df.groupby(&quot;year_of_release&quot;) .apply(lambda x: (x / x.sum()) * 100) .rename(&quot;market_share&quot;) .reset_index() ) . It looks like for platforms that have had a market share of 20% or more at some point, have on average is 3.2 years . successful_plat_years = ( market_share_platform.query(&quot;market_share &gt;= 20&quot;) .groupby(&quot;platform&quot;) .size() .sort_values(ascending=False) ) sns.kdeplot(successful_plat_years.values) plt.xlim(0) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Probability Density&quot;) plt.title(&quot;Number of years platform has acheived more than 20% market share&quot;) plt.show() . successful_plat_years.median() . 2.0 . On average, modern platforms hold a market share of 20% for 2 years . With this information, we can target platforms that have attained a market share of more than 20% for the first time in 2016, and consider those platforms having a high potential to maintain that market share next year. . sns.lineplot( data=market_share_platform, x=&quot;year_of_release&quot;, y=&quot;market_share&quot;, hue=&quot;platform&quot; ) plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0) . &lt;matplotlib.legend.Legend at 0x11c81c400&gt; . market_share_platform.query(&quot;market_share &gt;= 20&quot;) . year_of_release platform market_share . 3 2010.0 | PS3 | 31.050228 | . 6 2010.0 | X360 | 25.570776 | . 11 2011.0 | PS3 | 20.168067 | . 24 2012.0 | X360 | 25.144509 | . 29 2013.0 | PS4 | 38.012422 | . 39 2014.0 | PS4 | 27.334852 | . 45 2014.0 | XOne | 20.045558 | . 55 2015.0 | XOne | 20.675105 | . 61 2016.0 | Wii | 21.176471 | . 62 2016.0 | WiiU | 25.294118 | . Both the Wii and WiiU have yet to experience a second high market share event. I would consider investing in these platforms for 2017. . df[&quot;year_of_release&quot;] = pd.to_datetime(df[&quot;year_of_release&quot;], format=&quot;%Y&quot;) . g = ( df.groupby([&quot;year_of_release&quot;, &quot;platform&quot;])[&quot;total_sales&quot;] .median() .reset_index(&quot;platform&quot;) ) . g.pivot(columns=&quot;platform&quot;, values=&quot;total_sales&quot;) . platform 3DS DS PC PS2 PS3 PS4 PSP PSV Wii WiiU X360 XOne . year_of_release . 2010-01-01 NaN | 0.11 | 0.060 | 0.055 | 0.340 | NaN | 0.060 | NaN | 0.190 | NaN | 0.280 | NaN | . 2011-01-01 0.145 | 0.08 | 0.080 | 0.060 | 0.240 | NaN | 0.060 | 0.130 | 0.170 | NaN | 0.225 | NaN | . 2012-01-01 0.190 | 0.03 | 0.120 | NaN | 0.305 | NaN | 0.040 | 0.190 | 0.190 | 0.230 | 0.435 | NaN | . 2013-01-01 0.100 | 0.15 | 0.175 | NaN | 0.310 | 1.530 | 0.025 | 0.100 | 0.185 | 0.220 | 0.430 | 0.800 | . 2014-01-01 0.090 | NaN | 0.100 | NaN | 0.160 | 0.600 | 0.010 | 0.065 | 0.370 | 0.130 | 0.230 | 0.440 | . 2015-01-01 0.090 | NaN | 0.080 | NaN | 0.050 | 0.180 | 0.020 | 0.030 | 0.090 | 0.220 | 0.180 | 0.245 | . 2016-01-01 0.080 | NaN | 0.035 | NaN | 0.065 | 0.085 | NaN | 0.030 | 0.180 | 0.215 | 0.100 | 0.060 | . It looks like sales have stopped for certain systems; likely because a next generation platform overtook it. Certain platforms won&#39;t help us understand future sales like DS and PS2 . Furthermore, looking at our plots, it looks as if the Wii and Wiiu are experiencing very good sales in 2016. Futhermore, it looks like PS4 could experience a revival after a steep decline the previous year. . g.pivot(columns=&quot;platform&quot;, values=&quot;total_sales&quot;).drop([&quot;DS&quot;, &quot;PS2&quot;], axis=1).plot( figsize=(10, 10), sharex=True, sharey=True, logy=True ) plt.title(&quot;Total Sales by platform from 2010 to 2016&quot;) plt.ylabel(&quot;Total Sales USD Millions&quot;) plt.show() . Looking at platform that have been sucessful over the past 6 years, we can see that the X360, PS3, XOne, WiiU, Wii, and PS4 are the top contenders. . col_order = ( df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;platform&quot;, y=&quot;total_sales&quot;, order=col_order) plt.yscale(&quot;log&quot;) plt.show() . It looks like critic and user score has a strong correleation. . And both log total sales and scores have a positive correlation, though critic scores seem to have a stronger relationship. . df[&quot;log_total_sales&quot;] = np.log(df[&quot;total_sales&quot;]) sns.pairplot( data=df[[&quot;log_total_sales&quot;, &quot;critic_score&quot;, &quot;user_score&quot;]], corner=True, kind=&quot;reg&quot; ) plt.show() . mult_plat = df[&quot;name&quot;].duplicated(keep=False) mp_df = df.query(&quot;@mult_plat&quot;) col_order = ( mp_df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;platform&quot;, y=&quot;total_sales&quot;, order=col_order) plt.yscale(&quot;log&quot;) plt.ylabel(&quot;Total Sales in Millions USD&quot;) plt.show() . col_order = ( mp_df.groupby(&quot;genre&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) plt.figure(figsize=(10, 10)) sns.boxplot(data=df, x=&quot;genre&quot;, y=&quot;total_sales&quot;, order=col_order) plt.xticks(rotation=45) plt.yscale(&quot;log&quot;) plt.ylabel(&quot;Total Sales in Millions USD&quot;) plt.show() . Shooters seem to be the most profitable over the span we have selected. . df.columns . Index([&#39;name&#39;, &#39;platform&#39;, &#39;year_of_release&#39;, &#39;genre&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;, &#39;critic_score&#39;, &#39;user_score&#39;, &#39;rating&#39;, &#39;total_sales&#39;, &#39;log_total_sales&#39;], dtype=&#39;object&#39;) . User profile for each region . regions = [&quot;na_sales&quot;, &quot;eu_sales&quot;, &quot;jp_sales&quot;] reg_sale_df = pd.melt( df, [&quot;name&quot;, &quot;platform&quot;, &quot;genre&quot;, &quot;rating&quot;], regions, &quot;region&quot;, &quot;sales&quot; ) . reg_sale_df.groupby([&quot;region&quot;, &quot;platform&quot;])[&quot;sales&quot;].median() . region platform eu_sales 3DS 0.00 DS 0.00 PC 0.05 PS2 0.00 PS3 0.05 PS4 0.08 PSP 0.00 PSV 0.00 Wii 0.02 WiiU 0.07 X360 0.08 XOne 0.07 jp_sales 3DS 0.05 DS 0.00 PC 0.00 PS2 0.01 PS3 0.02 PS4 0.01 PSP 0.03 PSV 0.03 Wii 0.00 WiiU 0.00 X360 0.00 XOne 0.00 na_sales 3DS 0.01 DS 0.05 PC 0.00 PS2 0.00 PS3 0.09 PS4 0.06 PSP 0.00 PSV 0.00 Wii 0.11 WiiU 0.11 X360 0.16 XOne 0.12 Name: sales, dtype: float64 . reg_sale_df . name platform genre rating region sales . 0 Kinect Adventures! | X360 | Misc | E | na_sales | 15.00 | . 1 Grand Theft Auto V | PS3 | Action | M | na_sales | 7.02 | . 2 Grand Theft Auto V | X360 | Action | M | na_sales | 9.66 | . 3 Pokemon Black/Pokemon White | DS | Role-Playing | NaN | na_sales | 5.51 | . 4 Call of Duty: Modern Warfare 3 | X360 | Shooter | M | na_sales | 9.04 | . ... ... | ... | ... | ... | ... | ... | . 15826 Strawberry Nauts | PSV | Adventure | NaN | jp_sales | 0.01 | . 15827 Aiyoku no Eustia | PSV | Misc | NaN | jp_sales | 0.01 | . 15828 Samurai Warriors: Sanada Maru | PS3 | Action | NaN | jp_sales | 0.01 | . 15829 Haitaka no Psychedelica | PSV | Adventure | NaN | jp_sales | 0.01 | . 15830 Winning Post 8 2016 | PSV | Simulation | NaN | jp_sales | 0.01 | . 15831 rows × 6 columns . col_order = ( df.groupby(&quot;platform&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;platform&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . North American and European Sales seem to follow similar trends in platform for the most part. Japanese sales strongly differ and are more focused on PSP, PSV and 3DS. PC and PS4 games seem to have higher sales in Europe. . col_order = ( df.groupby(&quot;genre&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;genre&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . Most sales look pretty similar at first glance. There&#39;s a large difference in role playing games, where Japan has seen the most sales. . 41% of values for rating are missing, which makes imputation a challenge and will reduce the sample size that we&#39;re analyzing, that being said, lets look at how ratings affect sales per region. . col_order = ( df.groupby(&quot;rating&quot;)[&quot;total_sales&quot;] .median() .sort_values(ascending=False) .index.to_list() ) g = sns.catplot( kind=&quot;box&quot;, data=reg_sale_df, col=&quot;rating&quot;, x=&quot;region&quot;, y=&quot;sales&quot;, col_wrap=3, col_order=col_order, ) plt.ylim(0, 1.25) plt.show() . I suspect the rating systems aren&#39;t the same over all regions, so it may be difficult to make any conclusions with this dataset. . Hypothesis Testing . Average user ratings of the Xbox One and PC platforms are the same. . hypo = &quot;Average user ratings of the Xbox One and PC platforms are the same.&quot; pc_scores = df.loc[df[&quot;platform&quot;] == &quot;PC&quot;, &quot;user_score&quot;].dropna() xone_scores = df.loc[df[&quot;platform&quot;] == &quot;XOne&quot;, &quot;user_score&quot;].dropna() alpha = 0.05 result = stats.ttest_ind(pc_scores, xone_scores) if result.pvalue &lt; 0.05: print( f&quot;P value is close to {result.pvalue:.5f}. Reject the null hypothesis: n n{hypo}&quot; ) else: print( f&quot;P value is close to {result.pvalue:.5f}. Fail to reject the null hypothesis: n n{hypo}&quot; ) . P value is close to 0.98100. Fail to reject the null hypothesis: Average user ratings of the Xbox One and PC platforms are the same. . Average user ratings for the Action and Sports genres are different. . hypo = &quot;Average user ratings for the Action and Sports genres are different.&quot; action_scores = df.loc[df[&quot;genre&quot;] == &quot;Action&quot;, &quot;user_score&quot;].dropna() sports_scores = df.loc[df[&quot;genre&quot;] == &quot;Sports&quot;, &quot;user_score&quot;].dropna() alpha = 0.05 result = stats.ttest_ind(action_scores, sports_scores) if result.pvalue &lt; 0.05: print( f&quot;P value is close to {result.pvalue:.5f}. Reject the null hypothesis: n{hypo}&quot; ) else: print( f&quot;P value is close to {result.pvalue:.5f}. Fail to reject the null hypothesis: n {hypo}&quot; ) . P value is close to 0.00000. Reject the null hypothesis: Average user ratings for the Action and Sports genres are different. . Conclusion . We do notice major differences in sales when stratifying between platforms, genre, and region. Here are the major take aways. . Shooters, Platformers, Sports, Fighting, and Racing are the top selling genres on average from our modern games. | Wii and WiiU may be poised to take on a large market share next year | Generally, Critic Scores are a good indicator on how well a game will sell | Action and Sports games very likely do not share a distribution for sales. Action games are better sellers generally in this timeframe. | Japanese sales don&#39;t share the same trends compared to North American and European Sales. | .",
            "url": "https://jordwil.github.io/ds_blog/2020/04/28/integrated-project.html",
            "relUrl": "/2020/04/28/integrated-project.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Research on apartment sales ads",
            "content": "Examine file . #collapse import pandas as pd import matplotlib.pyplot as plt import numpy as np from pathlib import Path import missingno as msno from scipy import signal import seaborn as sns . . #collapse from pathlib import Path file = Path(&quot;../datasets/real_estate_data_eng.csv&quot;) df = pd.read_csv(file, sep=&quot; t&quot;) df.head() . . total_images last_price total_area first_day_exposition rooms ceiling_height floors_total living_area floor is_apartment ... kitchen_area balcony locality_name airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . 0 20 | 13000000.0 | 108.0 | 2019-03-07T00:00:00 | 3 | 2.70 | 16.0 | 51.0 | 8 | NaN | ... | 25.0 | NaN | Saint Peterburg | 18863.0 | 16028.0 | 1.0 | 482.0 | 2.0 | 755.0 | NaN | . 1 7 | 3350000.0 | 40.4 | 2018-12-04T00:00:00 | 1 | NaN | 11.0 | 18.6 | 1 | NaN | ... | 11.0 | 2.0 | Shushary village | 12817.0 | 18603.0 | 0.0 | NaN | 0.0 | NaN | 81.0 | . 2 10 | 5196000.0 | 56.0 | 2015-08-20T00:00:00 | 2 | NaN | 5.0 | 34.3 | 4 | NaN | ... | 8.3 | 0.0 | Saint Peterburg | 21741.0 | 13933.0 | 1.0 | 90.0 | 2.0 | 574.0 | 558.0 | . 3 0 | 64900000.0 | 159.0 | 2015-07-24T00:00:00 | 3 | NaN | 14.0 | NaN | 9 | NaN | ... | NaN | 0.0 | Saint Peterburg | 28098.0 | 6800.0 | 2.0 | 84.0 | 3.0 | 234.0 | 424.0 | . 4 2 | 10000000.0 | 100.0 | 2018-06-19T00:00:00 | 2 | 3.03 | 14.0 | 32.0 | 13 | NaN | ... | 41.0 | NaN | Saint Peterburg | 31856.0 | 8098.0 | 2.0 | 112.0 | 1.0 | 48.0 | 121.0 | . 5 rows × 22 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23699 entries, 0 to 23698 Data columns (total 22 columns): # Column Non-Null Count Dtype -- -- 0 total_images 23699 non-null int64 1 last_price 23699 non-null float64 2 total_area 23699 non-null float64 3 first_day_exposition 23699 non-null object 4 rooms 23699 non-null int64 5 ceiling_height 14504 non-null float64 6 floors_total 23613 non-null float64 7 living_area 21796 non-null float64 8 floor 23699 non-null int64 9 is_apartment 2775 non-null object 10 studio 23699 non-null bool 11 open_plan 23699 non-null bool 12 kitchen_area 21421 non-null float64 13 balcony 12180 non-null float64 14 locality_name 23650 non-null object 15 airports_nearest 18157 non-null float64 16 cityCenters_nearest 18180 non-null float64 17 parks_around3000 18181 non-null float64 18 parks_nearest 8079 non-null float64 19 ponds_around3000 18181 non-null float64 20 ponds_nearest 9110 non-null float64 21 days_exposition 20518 non-null float64 dtypes: bool(2), float64(14), int64(3), object(3) memory usage: 3.7+ MB . ((df.isna().mean() * 100).round(2).rename(&quot;Percent Missing&quot;).sort_values()) . total_images 0.00 open_plan 0.00 floor 0.00 rooms 0.00 studio 0.00 total_area 0.00 last_price 0.00 first_day_exposition 0.00 locality_name 0.21 floors_total 0.36 living_area 8.03 kitchen_area 9.61 days_exposition 13.42 ponds_around3000 23.28 parks_around3000 23.28 cityCenters_nearest 23.29 airports_nearest 23.38 ceiling_height 38.80 balcony 48.61 ponds_nearest 61.56 parks_nearest 65.91 is_apartment 88.29 Name: Percent Missing, dtype: float64 . df[[&quot;parks_around3000&quot;, &quot;parks_nearest&quot;]].query(&quot;parks_around3000 == 0&quot;) . parks_around3000 parks_nearest . 1 0.0 | NaN | . 6 0.0 | NaN | . 7 0.0 | NaN | . 9 0.0 | NaN | . 11 0.0 | NaN | . ... ... | ... | . 23684 0.0 | NaN | . 23685 0.0 | NaN | . 23687 0.0 | NaN | . 23688 0.0 | NaN | . 23691 0.0 | NaN | . 10106 rows × 2 columns . df[[&quot;parks_around3000&quot;, &quot;parks_nearest&quot;]].query(&quot;parks_nearest.isnull()&quot;).groupby( &quot;parks_around3000&quot; ).size() . parks_around3000 0.0 10102 dtype: int64 . Only zero values when parks around 3000 is null value. I&#39;ll reassign parks_nearest to 0 . msno.matrix(df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x106a36d00&gt; . msno.heatmap(df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x115de8730&gt; . df.columns . Index([&#39;total_images&#39;, &#39;last_price&#39;, &#39;total_area&#39;, &#39;first_day_exposition&#39;, &#39;rooms&#39;, &#39;ceiling_height&#39;, &#39;floors_total&#39;, &#39;living_area&#39;, &#39;floor&#39;, &#39;is_apartment&#39;, &#39;studio&#39;, &#39;open_plan&#39;, &#39;kitchen_area&#39;, &#39;balcony&#39;, &#39;locality_name&#39;, &#39;airports_nearest&#39;, &#39;cityCenters_nearest&#39;, &#39;parks_around3000&#39;, &#39;parks_nearest&#39;, &#39;ponds_around3000&#39;, &#39;ponds_nearest&#39;, &#39;days_exposition&#39;], dtype=&#39;object&#39;) . df.describe() . total_images last_price total_area rooms ceiling_height floors_total living_area floor kitchen_area balcony airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . count 23699.000000 | 2.369900e+04 | 23699.000000 | 23699.000000 | 14504.000000 | 23613.000000 | 21796.000000 | 23699.000000 | 21421.000000 | 12180.000000 | 18157.000000 | 18180.000000 | 18181.000000 | 8079.000000 | 18181.000000 | 9110.000000 | 20518.000000 | . mean 9.858475 | 6.541549e+06 | 60.348651 | 2.070636 | 2.771499 | 10.673824 | 34.457852 | 5.892358 | 10.569807 | 1.150082 | 28793.672193 | 14191.277833 | 0.611408 | 490.804555 | 0.770255 | 517.980900 | 180.888634 | . std 5.682529 | 1.088701e+07 | 35.654083 | 1.078405 | 1.261056 | 6.597173 | 22.030445 | 4.885249 | 5.905438 | 1.071300 | 12630.880622 | 8608.386210 | 0.802074 | 342.317995 | 0.938346 | 277.720643 | 219.727988 | . min 0.000000 | 1.219000e+04 | 12.000000 | 0.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.300000 | 0.000000 | 0.000000 | 181.000000 | 0.000000 | 1.000000 | 0.000000 | 13.000000 | 1.000000 | . 25% 6.000000 | 3.400000e+06 | 40.000000 | 1.000000 | 2.520000 | 5.000000 | 18.600000 | 2.000000 | 7.000000 | 0.000000 | 18585.000000 | 9238.000000 | 0.000000 | 288.000000 | 0.000000 | 294.000000 | 45.000000 | . 50% 9.000000 | 4.650000e+06 | 52.000000 | 2.000000 | 2.650000 | 9.000000 | 30.000000 | 4.000000 | 9.100000 | 1.000000 | 26726.000000 | 13098.500000 | 0.000000 | 455.000000 | 1.000000 | 502.000000 | 95.000000 | . 75% 14.000000 | 6.800000e+06 | 69.900000 | 3.000000 | 2.800000 | 16.000000 | 42.300000 | 8.000000 | 12.000000 | 2.000000 | 37273.000000 | 16293.000000 | 1.000000 | 612.000000 | 1.000000 | 729.000000 | 232.000000 | . max 50.000000 | 7.630000e+08 | 900.000000 | 19.000000 | 100.000000 | 60.000000 | 409.700000 | 33.000000 | 112.000000 | 5.000000 | 84869.000000 | 65968.000000 | 3.000000 | 3190.000000 | 3.000000 | 1344.000000 | 1580.000000 | . Conclusion . The data file is tab separated in defiance of it&#39;s extention of csv. | A large proportion of is_apartment column missing. | . Data preprocessing . Missing Value Analysis . is_aparment . Around 88% of this data is missing, and it&#39;s unclear what that value may be, as both boolean values exist within the data. . df[&quot;is_apartment&quot;].dropna().astype(str).hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x115a55d00&gt; . I&#39;ve decided to drop this feature from my analysis, as i&#39;m not confident that imputation can &#39;rescue&#39; the distribution of this sample. . df = df.drop(&quot;is_apartment&quot;, axis=1) . parks_nearest and ponds_nearest . parks and ponds nearest columns contain a large proportion of missing data as well, with around 66% and 62% respectively. . df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;box&quot;) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;hist&quot;, subplots=True, bins=50) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1163e2130&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1173d85e0&gt;], dtype=object) . Interestingly, there are many outliers in the parks_nearest column, and that trend isn&#39;t present for ponds nearest both contain a long tail where a median imputation would be most appropriate in an attempt to represent the distribution of this sample. What does this distribution look like after imputation? . park_pond_imputation = dict(df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].median()) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]] = df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].fillna( park_pond_imputation ) df[[&quot;parks_nearest&quot;, &quot;ponds_nearest&quot;]].plot(kind=&quot;hist&quot;, subplots=True, bins=50) # df[&#39;first_day_exposition&#39;].head() . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11648a070&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11758a430&gt;], dtype=object) . It seems strange to impute such a large proportion of the data. We will leave it in for now. . balcony, floors_total, living_area, kitchen_area, airports_nearest, cityCenters_nearest, parks_around3000, ponds_around3000, days_exposition, ceiling_height . Due to the rough poisson distribtion, the tails are going to be uneven, and a median may be the best measure of central tendancy. . df[&quot;balcony&quot;].fillna(df[&quot;balcony&quot;].median(), inplace=True) df[&quot;balcony&quot;].hist() plt.show() . df[&quot;floors_total&quot;].fillna(df[&quot;floors_total&quot;].median(), inplace=True) df[&quot;floors_total&quot;].hist(bins=25) plt.show() . Mode seems to be the most appropriate measure to fill the missing values in balcony. . df[&quot;living_area&quot;].fillna(df[&quot;living_area&quot;].median(), inplace=True) df[&quot;living_area&quot;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x117652eb0&gt; . df[&quot;kitchen_area&quot;].fillna(df[&quot;kitchen_area&quot;].median(), inplace=True) df[&quot;kitchen_area&quot;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x115b7dcd0&gt; . df[&quot;airports_nearest&quot;].fillna(df[&quot;airports_nearest&quot;].median(), inplace=True) df[&quot;airports_nearest&quot;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x117b2b790&gt; . df[&quot;cityCenters_nearest&quot;].fillna(df[&quot;cityCenters_nearest&quot;].median(), inplace=True) df[&quot;cityCenters_nearest&quot;].hist(bins=36) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x118156a30&gt; . df[&quot;parks_around3000&quot;].fillna(df[&quot;parks_around3000&quot;].median(), inplace=True) df[&quot;parks_around3000&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x118224ee0&gt; . df[&quot;ponds_around3000&quot;].fillna(df[&quot;ponds_around3000&quot;].median(), inplace=True) df[&quot;ponds_around3000&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ef22a30&gt; . df[&quot;days_exposition&quot;].fillna(df[&quot;days_exposition&quot;].median(), inplace=True) df[&quot;days_exposition&quot;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ee622e0&gt; . df[&quot;ceiling_height&quot;].plot(kind=&quot;box&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f238100&gt; . Somone has quite the ceiling! . df[&quot;ceiling_height&quot;].fillna(df[&quot;ceiling_height&quot;].median(), inplace=True) df[&quot;ceiling_height&quot;].hist(bins=40, range=(0, 20)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f1c61c0&gt; . Locality name . Let&#39;s fill in NA values with &#39;Unknown&#39; string. . df[&quot;locality_name&quot;].fillna(&quot;Unknown&quot;, inplace=True) df[&quot;locality_name&quot;].value_counts().head(10).plot(kind=&quot;bar&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f4066d0&gt; . Let&#39;s make sure we took care of all na values . df.isna().sum() . total_images 0 last_price 0 total_area 0 first_day_exposition 0 rooms 0 ceiling_height 0 floors_total 0 living_area 0 floor 0 studio 0 open_plan 0 kitchen_area 0 balcony 0 locality_name 0 airports_nearest 0 cityCenters_nearest 0 parks_around3000 0 parks_nearest 0 ponds_around3000 0 ponds_nearest 0 days_exposition 0 dtype: int64 . We can convert first_day_exposition to a datetime dtype . df[&quot;first_day_exposition&quot;] = pd.to_datetime( df[&quot;first_day_exposition&quot;], format=&quot;%Y-%m-%dT%H:%M:%S&quot; ) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23699 entries, 0 to 23698 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 total_images 23699 non-null int64 1 last_price 23699 non-null float64 2 total_area 23699 non-null float64 3 first_day_exposition 23699 non-null datetime64[ns] 4 rooms 23699 non-null int64 5 ceiling_height 23699 non-null float64 6 floors_total 23699 non-null float64 7 living_area 23699 non-null float64 8 floor 23699 non-null int64 9 studio 23699 non-null bool 10 open_plan 23699 non-null bool 11 kitchen_area 23699 non-null float64 12 balcony 23699 non-null float64 13 locality_name 23699 non-null object 14 airports_nearest 23699 non-null float64 15 cityCenters_nearest 23699 non-null float64 16 parks_around3000 23699 non-null float64 17 parks_nearest 23699 non-null float64 18 ponds_around3000 23699 non-null float64 19 ponds_nearest 23699 non-null float64 20 days_exposition 23699 non-null float64 dtypes: bool(2), datetime64[ns](1), float64(14), int64(3), object(1) memory usage: 3.5+ MB . Type conversions . Which float values should be int values? . We can determine this by looking for columns that can be represented by an integer type without losing information. . decimal_vals = ((df.loc[:, df.dtypes == float] % 1) != 0).sum().sort_values() replace_types = decimal_vals.map({0: &quot;int64&quot;}).dropna() decimal_vals . last_price 0 floors_total 0 balcony 0 airports_nearest 0 parks_around3000 0 parks_nearest 0 ponds_around3000 0 ponds_nearest 0 days_exposition 3997 cityCenters_nearest 5519 living_area 10978 total_area 11249 kitchen_area 13776 ceiling_height 22509 dtype: int64 . Based on the data from the code book, days_exposition should be represented as an integer. . decimal_vals.drop(&quot;days_exposition&quot;, inplace=True) . df = df.astype(replace_types.to_dict()).assign( days_exposition=df[&quot;days_exposition&quot;].round().astype(int) ) df.head() . total_images last_price total_area first_day_exposition rooms ceiling_height floors_total living_area floor studio ... kitchen_area balcony locality_name airports_nearest cityCenters_nearest parks_around3000 parks_nearest ponds_around3000 ponds_nearest days_exposition . 0 20 | 13000000 | 108.0 | 2019-03-07 | 3 | 2.70 | 16 | 51.0 | 8 | False | ... | 25.0 | 1 | Saint Peterburg | 18863 | 16028.0 | 1 | 482 | 2 | 755 | 95 | . 1 7 | 3350000 | 40.4 | 2018-12-04 | 1 | 2.65 | 11 | 18.6 | 1 | False | ... | 11.0 | 2 | Shushary village | 12817 | 18603.0 | 0 | 455 | 0 | 502 | 81 | . 2 10 | 5196000 | 56.0 | 2015-08-20 | 2 | 2.65 | 5 | 34.3 | 4 | False | ... | 8.3 | 0 | Saint Peterburg | 21741 | 13933.0 | 1 | 90 | 2 | 574 | 558 | . 3 0 | 64900000 | 159.0 | 2015-07-24 | 3 | 2.65 | 14 | 30.0 | 9 | False | ... | 9.1 | 0 | Saint Peterburg | 28098 | 6800.0 | 2 | 84 | 3 | 234 | 424 | . 4 2 | 10000000 | 100.0 | 2018-06-19 | 2 | 3.03 | 14 | 32.0 | 13 | False | ... | 41.0 | 1 | Saint Peterburg | 31856 | 8098.0 | 2 | 112 | 1 | 48 | 121 | . 5 rows × 21 columns . Transformations . Assign: Price per square meter . df[&quot;price_per_sq_meter&quot;] = df[&quot;last_price&quot;] / df[&quot;total_area&quot;] . Assign: day of week, month, and year. . df[&quot;day_of_week&quot;] = df[&quot;first_day_exposition&quot;].dt.dayofweek df[&quot;month&quot;] = df[&quot;first_day_exposition&quot;].dt.month df[&quot;year&quot;] = df[&quot;first_day_exposition&quot;].dt.year . Assign : floor type (top, bottom, other) . def gen_floor_type(x): if x[&quot;floors_total&quot;] == 1: return &quot;bottom&quot; elif x[&quot;floors_total&quot;] == x[&quot;floor&quot;]: return &quot;top&quot; else: return &quot;other&quot; df = df.assign(floor_type=df.apply(gen_floor_type, axis=1)) . Assign: living space to total area ratio, kitchen space to total area ratio . df[&quot;living_space_ratio&quot;] = df[&quot;living_area&quot;] / df[&quot;total_area&quot;] df[&quot;kitchen_space_ratio&quot;] = df[&quot;kitchen_area&quot;] / df[&quot;total_area&quot;] . EDA . Examining total_area, last_price, rooms, and ceiling_height. . Plotting a histogram is a great way to understand the distribution of a feature. . We will try to use these features to help understand how price relates to total area, number of rooms, and ceiling height. . cols_of_interest = [&quot;total_area&quot;, &quot;last_price&quot;, &quot;rooms&quot;, &quot;ceiling_height&quot;] pd.plotting.scatter_matrix(df[cols_of_interest], figsize=(10, 10)) plt.show() . df[cols_of_interest].describe() . Examining the numer of days it takes to sell an apartment . df[&quot;days_exposition&quot;].hist(bins=50) . df[&quot;days_exposition&quot;].describe() . df[&quot;days_exposition&quot;].plot(kind=&quot;box&quot;) . The average time it takes to complete a sale is around 95 days. The interquartile range allows us to determine what are considered unusally quick or long sales. Anything less than 45 days or more than 199 days is outside the interquartile range. . Removal of rare and outlying values. . I&#39;ll remove outlying values for our columns of interest in order to have a better understanding of how these features will affect the price. . Density of names is quite small for most here. I&#39;ll drop the 250 rarest localities. . def get_iqr(series): iqr = series.quantile(0.75) - series.quantile(0.25) return iqr def get_wiskers(series): low_out = series.quantile(0.25) - (1.5 * get_iqr(series)) high_out = series.quantile(0.75) + (1.5 * get_iqr(series)) return low_out, high_out def mask_outliers(series): return series.where(series.between(*get_wiskers(series))) . Let&#39;s test this function: . df[cols_of_interest].apply(get_wiskers) . df[cols_of_interest].apply(mask_outliers) . Looks good! . pd.plotting.scatter_matrix( df[cols_of_interest].apply(mask_outliers), figsize=(10, 10), alpha=0.1 ) plt.show() . The trends are much clearer here. There&#39;s a clear coreleation between last price and total area as well as number of rooms, with the strongest linear correlation coupled to total area. . sns.heatmap(df[cols_of_interest].apply(mask_outliers).corr(), annot=True) . df[&quot;floor_type&quot;] . Let&#39;s add some more interesting features to our analysis. . df[[&quot;cityCenters_nearest&quot;, &quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]].describe() . Let&#39;s remove outliers from cityCenters_nearest as well . cols_of_interest = cols_of_interest + [&quot;cityCenters_nearest&quot;] . Remove outliers from analysis . pd.plotting.scatter_matrix( df[cols_of_interest] .apply(mask_outliers) .join(df[[&quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]]), figsize=(20, 20), alpha=0.2, ) plt.show() . plt.figure(figsize=(10, 10)) sns.heatmap( df[cols_of_interest] .apply(mask_outliers) .join(df[[&quot;day_of_week&quot;, &quot;month&quot;, &quot;year&quot;]]) .corr(), annot=True, ) plt.show() . df.set_index(&quot;first_day_exposition&quot;)[&quot;last_price&quot;].resample(&quot;y&quot;).median().plot() . Theres a slight correlation with proximity to city centers and no clear linear relationship between time related values recorded. . top10_localities = df[&quot;locality_name&quot;].value_counts().iloc[:10].index locality_analysis = df.query(&quot;locality_name in @top10_localities&quot;).pivot_table( index=[&quot;locality_name&quot;], values=[&quot;price_per_sq_meter&quot;], aggfunc=[&quot;median&quot;, &quot;count&quot;] ) locality_analysis.columns = [&quot;median_price_per_sq_meter&quot;, &quot;num_occurences&quot;] . locality_analysis.sort_values(&quot;median_price_per_sq_meter&quot;, ascending=False).assign( median_price_per_sq_meter=locality_analysis[&quot;median_price_per_sq_meter&quot;].round(2) ) . The highest median price per square meter is Saint Peterburg, and Vyborg being the most affordable. . df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[ [&quot;cityCenters_nearest&quot;, &quot;last_price&quot;] ].plot(kind=&quot;scatter&quot;, x=&quot;cityCenters_nearest&quot;, y=&quot;last_price&quot;) . index_range = ( df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[&quot;cityCenters_nearest&quot;] .agg([&quot;min&quot;, &quot;max&quot;]) .add([0, 1]) .astype(int) .tolist() ) downtown_interp = ( df.query(&#39;locality_name == &quot;Saint Peterburg&quot;&#39;)[[&quot;cityCenters_nearest&quot;]] .join((df[&quot;last_price&quot;] / df[&quot;cityCenters_nearest&quot;]).rename(&quot;pr_per_nearest_km&quot;)) .round({&quot;cityCenters_nearest&quot;: 0}) .astype({&quot;cityCenters_nearest&quot;: int}) .pivot_table( index=&quot;cityCenters_nearest&quot;, values=&quot;pr_per_nearest_km&quot;, aggfunc=&quot;median&quot; ) .reindex(range(*index_range)) .interpolate(method=&quot;spline&quot;, order=2) .diff() .rolling(window=300, win_type=&quot;triang&quot;) .mean() ) . peak_x = signal.find_peaks(downtown_interp.values.flatten(), height=200)[0] . downtown_interp.plot(style=&quot;b&quot;, xlim=(0, 3000)) for peak in downtown_interp.iloc[peak_x].index.tolist(): plt.axvline(peak) . There seem to be two areas where the price will sharply change. I&#39;m going to use 540 km as the downtown limit. . downtown_interp.iloc[peak_x] . downtown_limit = 574 city_center_df = df.query( &quot;cityCenters_nearest &lt; @downtown_limit &amp; locality_name == &#39;Saint Peterburg&#39;&quot; ) sns.heatmap( city_center_df[[&quot;total_area&quot;, &quot;last_price&quot;, &quot;rooms&quot;, &quot;ceiling_height&quot;]].corr(), annot=True, ) . Rooms and price have a strong correlation between total area. . city_center_price_df = city_center_df[ [&quot;last_price&quot;, &quot;rooms&quot;, &quot;floor&quot;, &quot;cityCenters_nearest&quot;, &quot;first_day_exposition&quot;] ] . sns.heatmap(city_center_price_df.corr(), annot=True) . city_center_price_df.set_index(&quot;first_day_exposition&quot;)[&quot;last_price&quot;].resample( &quot;y&quot; ).median().plot() . It seems like 2016 had prices that were very low, and steadily increased each year. . Step 5. Overall conclusion . There doesn&#39;t seem to be a dramatic change in overall trends when comparing the instances near St. Petersburg city center and the entire dataset. . It most dramatic trend seems to be how publication year and price trended. There seems to be a much stronger increase in median price per year in the St. Petersburg city center values from 2016 to 2019. .",
            "url": "https://jordwil.github.io/ds_blog/eda/jupyter/2020/02/20/eda-project.html",
            "relUrl": "/eda/jupyter/2020/02/20/eda-project.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jordwil.github.io/ds_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jordwil.github.io/ds_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}